import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import pydeck as pdk
import requests
from bs4 import BeautifulSoup
import time
import urllib.parse
import re

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ“–", layout="wide")

st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå®Œå…¨ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒª")
st.markdown("Webãƒšãƒ¼ã‚¸ã‚’åˆ†å‰²ã—ã¦èª­ã¿è¾¼ã¿ã€**æ‰‹æŒã¡ã®CSVã«ãªã„æ–°ã—ã„æƒ…å ±ã®ã¿**ã‚’æ¼ã‚‰ã•ãšæŠ½å‡ºã—ã¾ã™ã€‚")

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def normalize_date(text):
    """æ—¥ä»˜ã‚’ã‚¼ãƒ­åŸ‹ã‚YYYYå¹´MMæœˆDDæ—¥å½¢å¼ã«çµ±ä¸€"""
    if not text: return text
    def replace_func(match):
        return f"{match.group(1)}å¹´{match.group(2).zfill(2)}æœˆ{match.group(3).zfill(2)}æ—¥"
    text = re.sub(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', replace_func, text)
    text = re.sub(r'(\d{4})/(\d{1,2})/(\d{1,2})', lambda m: f"{m.group(1)}/{m.group(2).zfill(2)}/{m.group(3).zfill(2)}", text)
    return text

def normalize_string(text):
    """
    æ–‡å­—åˆ—æ¯”è¼ƒç”¨ã®æ­£è¦åŒ–é–¢æ•°ï¼ˆæ¨æ¸¬ç”¨ï¼‰
    ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ã€å…¨è§’åŠè§’çµ±ä¸€ã€å°æ–‡å­—åŒ–ã‚’è¡Œã„ã€æºã‚‰ãã‚’å¸åã™ã‚‹
    """
    if not isinstance(text, str):
        return ""
    text = text.replace(" ", "").replace("ã€€", "")
    text = text.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return text.lower()

def split_text_into_chunks(text, chunk_size=15000, overlap=1000):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºã§åˆ†å‰²ã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    æƒ…å ±ã®åˆ†æ–­ã‚’é˜²ããŸã‚ã€overlapæ–‡å­—åˆ†ã ã‘å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’é‡è¤‡ã•ã›ã‚‹ã€‚
    """
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = start + chunk_size
        yield text[start:end]
        # æ¬¡ã®é–‹å§‹ä½ç½®ã¯ã€ç¾åœ¨ã®çµ‚äº†ä½ç½®ã‹ã‚‰overlapã‚’å¼•ã„ãŸå ´æ‰€ï¼ˆé‡è¤‡ã•ã›ã‚‹ï¼‰
        start = end - overlap

# --- Session State ---
if 'extracted_data' not in st.session_state:
    st.session_state.extracted_data = None
if 'last_update' not in st.session_state:
    st.session_state.last_update = None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("1. èª­ã¿è¾¼ã¿å¯¾è±¡")
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆã¯PRTIMESã®ã¿
    PRESET_URLS = {
        "PRTIMES (æœ€æ–°ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)": "https://prtimes.jp/"
    }
    
    selected_presets = st.multiselect(
        "ã‚µã‚¤ãƒˆã‚’é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (æœ€æ–°ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)"]
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area("ãã®ä»–ã®URL (1è¡Œã«1ã¤)", height=100, help="https://www.atpress.ne.jp/ ãªã©ã€è§£æã—ãŸã„ä»–ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.markdown("---")
    st.markdown("### 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿é™¤å¤– (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    uploaded_file = st.file_uploader("éå»ã«å–å¾—ã—ãŸCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv", help="ã“ã“ã«CSVã‚’ã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ã€ãã“ã«è¼‰ã£ã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã¯æ¤œç´¢çµæœã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™ï¼ˆå·®åˆ†ã®ã¿è¡¨ç¤ºï¼‰ã€‚")
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å‡¦ç†
    existing_fingerprints = set()
    if uploaded_file is not None:
        try:
            existing_df = pd.read_csv(uploaded_file)
            count = 0
            # CSVã®ã‚«ãƒ©ãƒ åãŒå¤šå°‘é•ã£ã¦ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«æ¢ã™
            name_col = next((col for col in existing_df.columns if 'ã‚¤ãƒ™ãƒ³ãƒˆå' in col or 'Name' in col), None)
            place_col = next((col for col in existing_df.columns if 'å ´æ‰€' in col or 'Place' in col), None)

            if name_col:
                for _, row in existing_df.iterrows():
                    n = normalize_string(row[name_col])
                    p = normalize_string(row[place_col]) if place_col else ""
                    # ã€Œã‚¤ãƒ™ãƒ³ãƒˆåã€ã¨ã€Œå ´æ‰€ã€ã®çµ„ã¿åˆã‚ã›ã‚’æŒ‡ç´‹ã¨ã—ã¦ç™»éŒ²
                    existing_fingerprints.add((n, p))
                    count += 1
                st.success(f"ğŸ“š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {count}ä»¶ ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯çµæœã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™ã€‚")
            else:
                st.error("CSVã«ã€Œã‚¤ãƒ™ãƒ³ãƒˆåã€ã¾ãŸã¯ã€ŒNameã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹ (å®Œå…¨ç¶²ç¾…ãƒ¢ãƒ¼ãƒ‰)", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})
    
    if custom_urls_text:
        for url in custom_urls_text.split('\n'):
            url = url.strip()
            if url and url.startswith("http"):
                domain = urllib.parse.urlparse(url).netloc
                targets.append({"url": url, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})
    
    unique_targets = {t['url']: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("âš ï¸ URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    all_data = []
    client = genai.Client(api_key=api_key)
    today = datetime.date.today()
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_urls = len(targets)
    
    skipped_count_duplicate_csv = 0
    
    # --- ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
    for i, target in enumerate(targets):
        url = target['url']
        label = target['label']
        
        status_text.info(f"â³ ({i+1}/{total_urls}) è§£æä¸­...: {label}")
        progress_bar.progress(i / total_urls)
        
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
            response = requests.get(url, headers=headers, timeout=15)
            response.encoding = response.apparent_encoding
            
            if response.status_code != 200:
                st.warning(f"âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {url}")
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            
            # ãƒã‚¤ã‚ºé™¤å»
            for script in soup(["script", "style", "nav", "footer", "iframe", "header", "noscript", "form"]):
                script.decompose()
            
            # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’å–å¾—ï¼ˆæœ€å¤§50ä¸‡æ–‡å­—ã¾ã§æ‹¡å¼µï¼‰
            full_text = soup.get_text(separator="\n", strip=True)[:500000]
            
            # --- â˜…ã“ã“ã‹ã‚‰åˆ†å‰²å‡¦ç† (Chunking) ---
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’15,000æ–‡å­—ãšã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹
            # â€» ä¸€åº¦ã«æŠ•ã’ã‚‹ã¨AIãŒé€”ä¸­ã‚’çœç•¥ã—ã¦ã—ã¾ã†ãŸã‚
            chunks = list(split_text_into_chunks(full_text, chunk_size=15000, overlap=1000))
            
            chunk_results = []
            
            # åˆ†å‰²ã—ãŸãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«AIã¸å•ã„åˆã‚ã›
            chunk_progress = st.progress(0)
            for cid, chunk_text in enumerate(chunks):
                # ã‚µãƒ–ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
                chunk_progress.progress((cid + 1) / len(chunks))
                
                prompt = f"""
                ã‚ãªãŸã¯å®Œç’§ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒã‚·ãƒ³ã§ã™ã€‚
                ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–­ç‰‡ï¼‰ã‹ã‚‰ã€å…¨ã¦ã®ã€Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€ã¾ãŸã¯ã€Œãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã€ã‚’æŠ½å‡ºã—ã€JSONãƒªã‚¹ãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                **çœç•¥ã¯ä¸€åˆ‡è¨±ã•ã‚Œã¾ã›ã‚“ã€‚äº›ç´°ãªæƒ…å ±ã‚‚å«ã‚ã€è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã¯å…¨ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚**

                ã€å‰ææƒ…å ±ã€‘
                ãƒ»æœ¬æ—¥ã®æ—¥ä»˜: {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}
                ãƒ»å‚ç…§URL: {url}
                
                ã€ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ï¼ˆæ–­ç‰‡ï¼‰ã€‘
                {chunk_text}

                ã€å³æ ¼ãªæŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
                1. ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã€Œã‚¤ãƒ™ãƒ³ãƒˆã€ã€Œæ–°å•†å“ã€ã€Œã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€ã€Œå±•ç¤ºä¼šã€ãªã©ã®æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚
                2. æ—¥ä»˜ã¯ã€ŒYYYYå¹´MMæœˆDDæ—¥ã€å½¢å¼ã€‚
                3. å ´æ‰€ï¼ˆlat, lonï¼‰ã¯å ´æ‰€åã‹ã‚‰æ¨æ¸¬ã™ã‚‹ã€‚
                4. æƒ…å ±ãŒãƒ†ã‚­ã‚¹ãƒˆå†…ã§å®Œçµã—ã¦ã„ãªã„ï¼ˆæ–‡ä¸­ã§åˆ‡ã‚Œã¦ã„ã‚‹ï¼‰å ´åˆã¯ã€ç„¡ç†ã«è£œå®Œã›ãšã€ç¢ºå®Ÿãªæƒ…å ±ã®ã¿æŠ½å‡ºã™ã‚‹ã€‚
                5. å‡ºåŠ›ã¯JSONã®ã¿ã€‚

                ã€å‡ºåŠ›å½¢å¼ã€‘
                [
                    {{
                        "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
                        "place": "é–‹å‚¬å ´æ‰€",
                        "date_info": "æœŸé–“",
                        "description": "æ¦‚è¦",
                        "lat": ç·¯åº¦(æ•°å€¤),
                        "lon": çµŒåº¦(æ•°å€¤)
                    }}
                ]
                """

                try:
                    ai_response = client.models.generate_content(
                        model="gemini-2.0-flash-exp",
                        contents=prompt,
                        config=types.GenerateContentConfig(
                            response_mime_type="application/json", 
                            temperature=0.0
                        )
                    )
                    
                    # JSONãƒ‘ãƒ¼ã‚¹
                    raw_json = ai_response.text.replace("```json", "").replace("```", "").strip()
                    extracted = json.loads(raw_json)
                    if isinstance(extracted, list):
                        chunk_results.extend(extracted)
                        
                except Exception as e:
                    # åˆ†å‰²ã®ä¸€éƒ¨ãŒå¤±æ•—ã—ã¦ã‚‚å…¨ä½“ã‚’æ­¢ã‚ãªã„
                    print(f"Chunk error: {e}")
                    continue
                
                time.sleep(1) # APIåˆ¶é™å›é¿ç”¨ã‚¦ã‚§ã‚¤ãƒˆ

            chunk_progress.empty() # ã‚µãƒ–ãƒãƒ¼æ¶ˆå»

            # --- åˆ†å‰²çµæœã®çµ±åˆã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯ ---
            seen_in_page = set()
            
            for item in chunk_results:
                # ãƒšãƒ¼ã‚¸å†…ã§ã®é‡è¤‡æ’é™¤ï¼ˆChunkã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å¯¾ç­–ï¼‰
                n_key = normalize_string(item.get('name', ''))
                if not n_key or n_key in seen_in_page:
                    continue
                seen_in_page.add(n_key)

                # CSVã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                p_key = normalize_string(item.get('place', ''))
                
                is_in_csv = False
                if (n_key, p_key) in existing_fingerprints:
                    is_in_csv = True
                elif p_key == "" and any(ef[0] == n_key for ef in existing_fingerprints):
                    is_in_csv = True
                
                if is_in_csv:
                    skipped_count_duplicate_csv += 1
                    continue

                # æ¡ç”¨
                item['source_label'] = label
                item['source_url'] = url
                if item.get('date_info'):
                    item['date_info'] = normalize_date(item['date_info'])
                all_data.append(item)

        except Exception as e:
            st.warning(f"ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {label} (ã‚¨ãƒ©ãƒ¼: {e})")
            continue

    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()

    if not all_data and skipped_count_duplicate_csv > 0:
        st.warning(f"ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—ã§ãã¾ã—ãŸãŒã€å…¨ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVã«å«ã¾ã‚Œã‚‹ã€Œæ—¢çŸ¥ã®æƒ…å ±ã€ã ã£ãŸãŸã‚ã€è¡¨ç¤ºã™ã‚‹ã‚‚ã®ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ï¼ˆé™¤å¤–æ•°: {skipped_count_duplicate_csv}ä»¶ï¼‰")
        st.session_state.extracted_data = None
    elif not all_data:
        st.error("æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.session_state.extracted_data = None
    else:
        # æœ€çµ‚çš„ãªãƒªã‚¹ãƒˆã®é‡è¤‡æ’é™¤ï¼ˆå¿µã®ç‚ºï¼‰
        unique_data = []
        seen_keys = set()
        for item in all_data:
            name_key = normalize_string(item.get('name', ''))
            place_key = normalize_string(item.get('place', ''))
            
            if (name_key, place_key) not in seen_keys:
                seen_keys.add((name_key, place_key))
                unique_data.append(item)
        
        st.session_state.extracted_data = unique_data
        st.session_state.last_update = datetime.datetime.now().strftime("%H:%M:%S")
        
        msg = f"ğŸ‰ èª­ã¿è¾¼ã¿å®Œäº†ï¼ æ–°è¦ {len(unique_data)} ä»¶"
        if skipped_count_duplicate_csv > 0:
            msg += f" (CSVã¨ã®é‡è¤‡ {skipped_count_duplicate_csv} ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸ)"
        status_text.success(msg)

# --- çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---

if st.session_state.extracted_data is not None:
    data = st.session_state.extracted_data
    df = pd.DataFrame(data)

    st.markdown(f"**æœ€çµ‚æ›´æ–°: {st.session_state.last_update}** ({len(data)}ä»¶)")

    # 1. ãƒãƒƒãƒ—è¡¨ç¤º
    st.subheader("ğŸ“ ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ— (æ–°è¦ã®ã¿)")
    if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
        map_df = df.dropna(subset=['lat', 'lon'])
        if not map_df.empty:
            view_state = pdk.ViewState(
                latitude=map_df['lat'].mean(),
                longitude=map_df['lon'].mean(),
                zoom=11,
                pitch=0,
            )
            layer = pdk.Layer(
                "ScatterplotLayer",
                map_df,
                get_position='[lon, lat]',
                get_color='[255, 75, 75, 160]',
                get_radius=300,
                pickable=True,
            )
            st.pydeck_chart(pdk.Deck(
                map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                initial_view_state=view_state,
                layers=[layer],
                tooltip={"html": "<b>{name}</b><br/>{place}<br/><i>{date_info}</i>"}
            ))

    # 2. ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    st.markdown("---")
    st.subheader("ğŸ“‹ æ–°è¦ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§")

    display_cols = ['date_info', 'name', 'place', 'description', 'source_label', 'source_url']
    available_cols = [c for c in display_cols if c in df.columns]
    display_df = df[available_cols].copy()
    
    rename_map = {
        'date_info': 'æœŸé–“', 'name': 'ã‚¤ãƒ™ãƒ³ãƒˆå', 'place': 'å ´æ‰€', 
        'description': 'æ¦‚è¦', 'source_label': 'æƒ…å ±æº', 'source_url': 'ãƒªãƒ³ã‚¯URL'
    }
    display_df = display_df.rename(columns=rename_map)

    try:
        display_df = display_df.sort_values('æœŸé–“')
    except:
        pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "ãƒªãƒ³ã‚¯URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— ãƒªãƒ³ã‚¯ã‚’é–‹ã"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large")
        },
        hide_index=True
    )

    # 3. CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = display_df.to_csv(index=False).encode('utf-8_sig')
    st.download_button(
        label="ğŸ“¥ æ–°è¦åˆ†CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="events_new_only.csv",
        mime='text/csv'
    )
