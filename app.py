import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import pydeck as pdk
import requests
from bs4 import BeautifulSoup
import time

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ“–")

st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œä¸€æ‹¬ç›´èª­ã€æŠ½å‡ºã‚¢ãƒ—ãƒª")
st.markdown("è¤‡æ•°ã®Webãƒšãƒ¼ã‚¸ã‚’é †ç•ªã«AIãŒèª­ã¿è¾¼ã¿ã€æƒ…å ±ã‚’çµ±åˆã—ã¦ãƒªã‚¹ãƒˆåŒ–ã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("èª­ã¿è¾¼ã¿å¯¾è±¡ (è¤‡æ•°é¸æŠå¯)")
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆURLãƒªã‚¹ãƒˆ
    PRESET_URLS = {
        "Walkerplus (ä»Šæ—¥ã®ã‚¤ãƒ™ãƒ³ãƒˆ/æ±äº¬)": "https://www.walkerplus.com/event_list/today/ar0300/",
        "Walkerplus (ä»Šé€±æœ«ã®ã‚¤ãƒ™ãƒ³ãƒˆ/æ±äº¬)": "https://www.walkerplus.com/event_list/weekend/ar0300/",
        "Walkerplus (æ¥é€±ã®ã‚¤ãƒ™ãƒ³ãƒˆ/æ±äº¬)": "https://www.walkerplus.com/event_list/next_week/ar0300/",
        "Let's Enjoy Tokyo (ç¾åœ¨é–‹å‚¬ä¸­/æ¸‹è°·)": "https://www.enjoytokyo.jp/event/list/chi03/?date_type=current",
        "Let's Enjoy Tokyo (ä»Šé€±æœ«/æ¸‹è°·)": "https://www.enjoytokyo.jp/event/list/chi03/?date_type=weekend",
        "Fashion Press (æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹)": "https://www.fashion-press.net/news/",
        "TimeOut Tokyo (æ±äº¬ã®ã‚¤ãƒ™ãƒ³ãƒˆ)": "https://www.timeout.jp/tokyo/ja/things-to-do"
    }
    
    # ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã«å¤‰æ›´
    selected_presets = st.multiselect(
        "ãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["Walkerplus (ä»Šæ—¥ã®ã‚¤ãƒ™ãƒ³ãƒˆ/æ±äº¬)"]
    )
    
    st.markdown("---")
    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area(
        "ãã®ä»–ã®URLï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã§è¤‡æ•°å…¥åŠ›å¯ï¼‰",
        placeholder="https://...\nhttps://...",
        height=100
    )

    st.info("ğŸ’¡ é¸æŠã—ãŸã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’é †ç•ªã«è§£æã—ã€çµæœã‚’1ã¤ã®ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    # URLãƒªã‚¹ãƒˆã®ä½œæˆ
    target_urls = []
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰è¿½åŠ 
    for label in selected_presets:
        target_urls.append(PRESET_URLS[label])
    
    # ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›ã‹ã‚‰è¿½åŠ 
    if custom_urls_text:
        for url in custom_urls_text.split('\n'):
            url = url.strip()
            if url and url.startswith("http"):
                target_urls.append(url)
    
    # é‡è¤‡é™¤å»
    target_urls = list(set(target_urls))

    if not target_urls:
        st.error("âš ï¸ URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    # å‡¦ç†é–‹å§‹
    all_data = []
    client = genai.Client(api_key=api_key)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_urls = len(target_urls)
    
    # --- URLã”ã¨ã®ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
    for i, url in enumerate(target_urls):
        current_progress = (i / total_urls)
        progress_bar.progress(current_progress)
        status_text.info(f"â³ ({i+1}/{total_urls}) ãƒšãƒ¼ã‚¸ã‚’è§£æä¸­... \n{url}")
        
        try:
            # 1. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—é•·ã‚ã«è¨­å®š
            response = requests.get(url, headers=headers, timeout=15)
            response.encoding = response.apparent_encoding
            
            if response.status_code != 200:
                st.warning(f"âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— (Status: {response.status_code}): {url}")
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            
            # ä¸è¦ã‚¿ã‚°å‰Šé™¤
            for script in soup(["script", "style", "nav", "footer", "iframe", "header"]):
                script.decompose()
                
            page_text = soup.get_text(separator="\n", strip=True)
            page_text = page_text[:40000] # æ–‡å­—æ•°åˆ¶é™

            # 2. AIè§£æ
            prompt = f"""
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€ã‚’æŠ½å‡ºã—ã€JSONå½¢å¼ã§ãƒªã‚¹ãƒˆåŒ–ã—ã¦ãã ã•ã„ã€‚

            ã€ãƒšãƒ¼ã‚¸URLã€‘
            {url}

            ã€ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã€‘
            {page_text}

            ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
            1. ã‚¤ãƒ™ãƒ³ãƒˆåã€æœŸé–“ã€å ´æ‰€ã€æ¦‚è¦ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            2. ãƒ†ã‚­ã‚¹ãƒˆã«æ›¸ã‹ã‚Œã¦ã„ãªã„æƒ…å ±ã¯å‰µä½œã›ãšã€ä¸æ˜ãªã‚‰ç©ºæ¬„ã«ã—ã¦ãã ã•ã„ã€‚
            3. å ´æ‰€ã®ç·¯åº¦çµŒåº¦ï¼ˆlat, lonï¼‰ã¯ã€å ´æ‰€åã‹ã‚‰æ¨æ¸¬ã—ã¦åŸ‹ã‚ã¦ãã ã•ã„ã€‚
            4. `source_url` ã«ã¯ã“ã®ãƒšãƒ¼ã‚¸ã®URL({url})ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚

            ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
            [
                {{
                    "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
                    "place": "é–‹å‚¬å ´æ‰€",
                    "date_info": "æœŸé–“",
                    "description": "æ¦‚è¦(ç°¡æ½”ã«)",
                    "source_url": "{url}",
                    "lat": ç·¯åº¦(æ•°å€¤),
                    "lon": çµŒåº¦(æ•°å€¤)
                }}
            ]
            """

            # å®‰å®šå‹•ä½œã®ãŸã‚ gemini-2.0-flash-exp ã‚’ä½¿ç”¨
            ai_response = client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.0
                )
            )
            
            # JSONå¤‰æ›
            text_resp = ai_response.text.replace("```json", "").replace("```", "").strip()
            extracted_list = json.loads(text_resp)
            
            # çµæœã‚’çµ±åˆãƒªã‚¹ãƒˆã«è¿½åŠ 
            if isinstance(extracted_list, list):
                all_data.extend(extracted_list)
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(1)

        except Exception as e:
            st.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {url}\nã‚¨ãƒ©ãƒ¼å†…å®¹: {e}")
            continue

    # --- å®Œäº†å‡¦ç† ---
    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()

    if not all_data:
        st.error("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    else:
        status_text.success(f"ğŸ‰ å®Œäº†ï¼ åˆè¨ˆ {len(all_data)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
    df = pd.DataFrame(all_data)

    # --- 1. ãƒãƒƒãƒ—è¡¨ç¤º (çµ±åˆç‰ˆ) ---
    st.subheader("ğŸ“ ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ— (å…¨ä»¶)")
    
    if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
        map_df = df.dropna(subset=['lat', 'lon'])
        if not map_df.empty:
            view_state = pdk.ViewState(
                latitude=map_df['lat'].mean(),
                longitude=map_df['lon'].mean(),
                zoom=11,
                pitch=0,
            )
            layer = pdk.Layer(
                "ScatterplotLayer",
                map_df,
                get_position='[lon, lat]',
                get_color='[255, 75, 75, 160]',
                get_radius=300,
                pickable=True,
            )
            st.pydeck_chart(pdk.Deck(
                map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                initial_view_state=view_state,
                layers=[layer],
                tooltip={
                    "html": "<b>{name}</b><br/>{place}<br/><i>{date_info}</i>",
                    "style": {"backgroundColor": "steelblue", "color": "white"}
                }
            ))
    
    # --- 2. ãƒªã‚¹ãƒˆè¡¨ç¤º ---
    st.markdown("---")
    st.subheader("ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ")
    
    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = df.to_csv(index=False).encode('utf-8_sig')
    st.download_button(
        label="ğŸ“¥ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="events_all_extracted.csv",
        mime='text/csv'
    )

    # ãƒªã‚¹ãƒˆè¡¨ç¤º
    for item in all_data:
        st.markdown(f"""
        - **æœŸé–“**: {item.get('date_info')}
        - **ã‚¤ãƒ™ãƒ³ãƒˆå**: {item.get('name')}
        - **å ´æ‰€**: {item.get('place')}
        - **æ¦‚è¦**: {item.get('description')}
        - [ğŸ”— æƒ…å ±å…ƒãƒšãƒ¼ã‚¸ã¸]({item.get('source_url')})
        """)
