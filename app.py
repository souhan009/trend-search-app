import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import re
import pydeck as pdk
import urllib.parse
import time # æ™‚é–“èª¿æ•´ç”¨

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ—ºï¸")

st.title("ğŸ—ºï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆMapæ¤œç´¢")
st.markdown("é«˜æ€§èƒ½AIãƒ¢ãƒ‡ãƒ«(Proç‰ˆ)ã‚’ä½¿ç”¨ã—ã€Webä¸Šã®è¨˜äº‹ã‚’æ™‚é–“ã‚’ã‹ã‘ã¦ç²¾æŸ»ã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    st.markdown("### ğŸ“ åœ°åŸŸãƒ»å ´æ‰€")
    region = st.text_input("æ¤œç´¢ã—ãŸã„å ´æ‰€", value="æ±äº¬éƒ½æ¸‹è°·åŒº", help="å…·ä½“çš„ãªåœ°åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.markdown("---")
    st.markdown("### ğŸŒ æ¤œç´¢å¯¾è±¡ã‚µã‚¤ãƒˆ")
    
    SITE_PATHS = {
        "Fashion Press (ãƒ‹ãƒ¥ãƒ¼ã‚¹)": "fashion-press.net/news/",
        "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹)": "walkerplus.com/article/",
        "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ)": "walkerplus.com/event_list/",
        "Let's Enjoy Tokyo (ã‚¤ãƒ™ãƒ³ãƒˆ)": "enjoytokyo.jp/event/",
        "TimeOut Tokyo (ã‚¬ã‚¤ãƒ‰)": "timeout.jp/tokyo/ja/things-to-do/",
        "PR TIMES (ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)": "prtimes.jp/main/html/rd/p/",
        "FASHIONSNAP (ãƒ‹ãƒ¥ãƒ¼ã‚¹)": "fashionsnap.com/article/"
    }
    
    selected_labels = st.multiselect(
        "æ¤œç´¢å¯¾è±¡ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
        options=list(SITE_PATHS.keys()),
        default=["Fashion Press (ãƒ‹ãƒ¥ãƒ¼ã‚¹)", "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹)", "Let's Enjoy Tokyo (ã‚¤ãƒ™ãƒ³ãƒˆ)"]
    )
    
    st.info("ğŸ’¡ ç²¾åº¦é‡è¦–ã®ã€ŒProãƒ¢ãƒ‡ãƒ«ã€ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€æ¤œç´¢ã«ã¯30ç§’ã€œ1åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("æ¤œç´¢é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    if not selected_labels:
        st.error("âš ï¸ æ¤œç´¢å¯¾è±¡ã‚’å°‘ãªãã¨ã‚‚1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # é€²æ—ãƒãƒ¼ã®è¡¨ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()

    # STEP 1: æº–å‚™ (10%)
    status_text.info("ğŸš€ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’èµ·å‹•ä¸­...")
    time.sleep(1)
    progress_bar.progress(10)
    
    client = genai.Client(api_key=api_key)
    target_paths = [SITE_PATHS[label] for label in selected_labels]
    site_query = " OR ".join([f"site:{path}" for path in target_paths])
    today = datetime.date.today()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (Proãƒ¢ãƒ‡ãƒ«å‘ã‘ã«ã€ã•ã‚‰ã«å³å¯†ãªæŒ‡ç¤ºã«å¤‰æ›´)
    prompt = f"""
    ã‚ãªãŸã¯ã€Œé«˜ç²¾åº¦ãªãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ãƒ»ãƒ­ãƒœãƒƒãƒˆã€ã§ã™ã€‚
    Googleæ¤œç´¢ã‚’è¡Œã„ã€ä»¥ä¸‹ã®æ¡ä»¶ã«åˆè‡´ã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æ…é‡ã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
    ã€Œ{region} ã‚¤ãƒ™ãƒ³ãƒˆ é–‹å‚¬ä¸­ {site_query}ã€
    ã€Œ{region} æ–°è¦ã‚ªãƒ¼ãƒ—ãƒ³ æ±ºå®š {site_query}ã€

    ã€åŸºæº–æ—¥ã€‘
    æœ¬æ—¥ã¯ {today} ã§ã™ã€‚çµ‚äº†æ¸ˆã¿ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

    ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šURLã®å®Ÿåœ¨ç¢ºèªã€‘
    1. **URLã®æ¨æ¸¬ãƒ»å‰µä½œã¯å³ç¦ã§ã™ã€‚** Walkerplusãªã©ã®è¨˜äº‹URLã«ã‚ã‚‹æ•°å­—IDï¼ˆä¾‹: article/12345/ï¼‰ã‚’å‹æ‰‹ã«å¤‰ãˆãŸã‚Šã€é©å½“ãªæ•°å­—ã‚’å…¥ã‚ŒãŸã‚Šã—ãªã„ã§ãã ã•ã„ã€‚
    2. **æ¤œç´¢çµæœã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã€Œãƒªãƒ³ã‚¯ãã®ã‚‚ã®ã€** ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    3. ã‚‚ã—è¨˜äº‹ã®å€‹åˆ¥URLãŒæ¤œç´¢çµæœã‹ã‚‰èª­ã¿å–ã‚Œãªã„å ´åˆã¯ã€ç„¡ç†ã«URLã‚’è²¼ã‚‰ãš `null` ã«ã—ã¦ãã ã•ã„ã€‚å˜˜ã®URLã‚’è²¼ã‚‹ã‚ˆã‚Šãƒã‚·ã§ã™ã€‚

    ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
    [
        {{
            "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
            "place": "é–‹å‚¬å ´æ‰€",
            "date_info": "æœŸé–“(ä¾‹: 11/1ã€œ12/25)",
            "description": "æ¦‚è¦(çŸ­ãã¦OK)",
            "source_name": "ã‚µã‚¤ãƒˆå",
            "url": "è¨˜äº‹ã®URL(å®Ÿåœ¨ã™ã‚‹ã‚‚ã®ã®ã¿)",
            "lat": ç·¯åº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null),
            "lon": çµŒåº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null)
        }}
    ]
    """

    # STEP 2: æ¤œç´¢å®Ÿè¡Œ (30%)
    status_text.info(f"ğŸ” {region}å‘¨è¾ºã®æƒ…å ±ã‚’æ¤œç´¢ä¸­... (Proãƒ¢ãƒ‡ãƒ«ã§è©³ç´°ã«è§£æã—ã¾ã™)")
    progress_bar.progress(30)

    # æ¤œç´¢å®Ÿè¡Œé–¢æ•°
    def execute_search(model_name):
        return client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
                response_mime_type="application/json",
                temperature=0.0 # å‰µé€ æ€§ã‚¼ãƒ­
            )
        )

    response = None
    
    try:
        # â˜…ã“ã“ã‚’å¤‰æ›´: gemini-1.5-pro-002 (é«˜æ€§èƒ½ãƒ»ä½é€Ÿãƒ¢ãƒ‡ãƒ«) ã‚’ä½¿ç”¨
        # ã“ã‚Œã«ã‚ˆã‚Šã€Œã¡ã‚ƒã‚“ã¨è€ƒãˆã¦ã€ã‹ã‚‰ç­”ãˆã‚’å‡ºã™ã‚ˆã†ã«ãªã‚Šã¾ã™
        response = execute_search("gemini-1.5-pro-002")
    except Exception as e:
        status_text.warning("âš ï¸ Proãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã—ãªã„ãŸã‚ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
        try:
            time.sleep(2)
            response = execute_search("gemini-1.5-flash-002")
        except Exception as e2:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e2}")
            st.stop()

    # STEP 3: ãƒ‡ãƒ¼ã‚¿ã®è§£æã¨æ¤œè¨¼ (80%)
    status_text.info("ğŸ“ å–å¾—ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã¨URLã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
    progress_bar.progress(80)
    time.sleep(1) # ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ã‚‹æ„Ÿã‚’æ¼”å‡ºï¼ˆå®Ÿéš›ã«ã¯ä»¥ä¸‹ã®å‡¦ç†æ™‚é–“ã¯çŸ­ã„ã®ã§ï¼‰

    # --- JSONãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º ---
    text = response.text.replace("```json", "").replace("```", "").strip()
    data = []
    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        try:
            if e.msg.startswith("Extra data"):
                data = json.loads(text[:e.pos])
            else:
                match = re.search(r'\[.*\]', text, re.DOTALL)
                if match:
                    data = json.loads(match.group(0))
        except:
            pass
    
    # --- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & URLç‰©ç†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
    cleaned_data = []
    for item in data:
        name = item.get('name', '')
        url = item.get('url', '')
        
        # 1. åå‰ãƒã‚§ãƒƒã‚¯
        if not name or name.lower() in ['unknown', 'ã‚¤ãƒ™ãƒ³ãƒˆ']:
            continue
        
        # 2. URLãƒã‚§ãƒƒã‚¯ (ãƒ‰ãƒ¡ã‚¤ãƒ³æŒ‡å®š + 404ã«ãªã‚ŠãŒã¡ãªãƒ‘ã‚¿ãƒ¼ãƒ³æ’é™¤)
        is_valid = False
        if url and url.startswith("http"):
            # è¨±å¯ã•ã‚ŒãŸãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
            for path in target_paths:
                # ãƒ‘ã‚¹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã ã‘ã§ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
                check_domain = path.split('/')[0] 
                if check_domain in url:
                    is_valid = True
                    break
        
        # â˜… Walkerplusã®å¹»è¦šURL (kankoã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ç­‰) ã‚’å†åº¦ç‰©ç†ãƒ–ãƒ­ãƒƒã‚¯
        if "kanko.walkerplus" in url:
            is_valid = False

        if not is_valid:
            # URLãŒæ€ªã—ã„ã€ã¾ãŸã¯nullã®å ´åˆã¯Googleæ¤œç´¢ãƒªãƒ³ã‚¯ã¸
            search_query = f"{item['name']} {item['place']} ã‚¤ãƒ™ãƒ³ãƒˆ"
            item['url'] = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
            item['source_name'] = "Googleæ¤œç´¢"
        
        cleaned_data.append(item)
        
    data = cleaned_data

    # STEP 4: å®Œäº† (100%)
    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty() # ãƒãƒ¼ã‚’æ¶ˆã™

    if not data:
        status_text.error("æ¡ä»¶ã«åˆã†è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.stop()
    else:
        status_text.success(f"æ¤œç´¢å®Œäº†ï¼ {len(data)}ä»¶ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")


    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
    df = pd.DataFrame(data)

    # --- 1. é«˜æ©Ÿèƒ½åœ°å›³ (Voyager) ---
    st.subheader(f"ğŸ“ {region}å‘¨è¾ºã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ—")
    st.caption(f"æŠ½å‡ºä»¶æ•°: {len(data)}ä»¶")
    
    if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
        map_df = df.dropna(subset=['lat', 'lon'])
        
        if not map_df.empty:
            view_state = pdk.ViewState(
                latitude=map_df['lat'].mean(),
                longitude=map_df['lon'].mean(),
                zoom=13,
                pitch=0,
            )

            layer = pdk.Layer(
                "ScatterplotLayer",
                map_df,
                get_position='[lon, lat]',
                get_color='[255, 75, 75, 160]',
                get_radius=200,
                pickable=True,
            )

            st.pydeck_chart(pdk.Deck(
                map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                initial_view_state=view_state,
                layers=[layer],
                tooltip={
                    "html": "<b>{name}</b><br/>{place}<br/><i>{description}</i>",
                    "style": {"backgroundColor": "steelblue", "color": "white"}
                }
            ))
            st.caption("â€»åœ°å›³ä¸Šã®èµ¤ã„ä¸¸ã«ãƒã‚¦ã‚¹ã‚’ä¹—ã›ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            
            # CSVä½œæˆ
            export_data = []
            for _, row in map_df.iterrows():
                gaiyou = f"ã€æœŸé–“ã€‘{row.get('date_info')}\n{row.get('description')}"
                export_data.append({
                    "Name": row.get('name'),
                    "ä½æ‰€": row.get('place'),
                    "æ¦‚è¦": gaiyou,
                    "å…¬å¼ã‚µã‚¤ãƒˆ": row.get('url', '')
                })
            
            export_df = pd.DataFrame(export_data)
            csv = export_df.to_csv(index=False).encode('utf-8_sig')

            st.download_button(
                label="ğŸ“¥ Googleãƒã‚¤ãƒãƒƒãƒ—ç”¨CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"event_map_{region}.csv",
                mime='text/csv',
                help="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Googleãƒã‚¤ãƒãƒƒãƒ—ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ã€Œä½æ‰€ã€åˆ—ã‚’ç›®å°ã®å ´æ‰€ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
            )
        else:
             st.info("â€»ä½ç½®æƒ…å ±ãŒç‰¹å®šã§ããªã‹ã£ãŸãŸã‚ã€åœ°å›³ã«ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ãŒã€ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã«ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚")
    else:
        st.warning("åœ°å›³ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- 2. é€Ÿå ±ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ ---
    st.markdown("---")
    st.subheader("ğŸ“‹ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ä¸€è¦§")
    
    for item in data:
        url_text = "ãªã—"
        source_label = item.get('source_name', 'æ²è¼‰ã‚µã‚¤ãƒˆ')
        
        link_label = f"{source_label} ã§è¦‹ã‚‹"
        if source_label == "Googleæ¤œç´¢":
            link_label = "ğŸ” Googleã§å†æ¤œç´¢"

        if item.get('url'):
            url_text = f"[ğŸ”— {link_label}]({item.get('url')})"

        st.markdown(f"""
        - **æœŸé–“**: {item.get('date_info')}
        - **ã‚¤ãƒ™ãƒ³ãƒˆå**: {item.get('name')}
        - **å ´æ‰€**: {item.get('place')}
        - **æ¦‚è¦**: {item.get('description')}
        - **ã‚½ãƒ¼ã‚¹**: {url_text}
        """)
