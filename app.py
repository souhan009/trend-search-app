import streamlit as st
import datetime
import os
import json
import time
import re
import urllib.parse
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set, Any

import pandas as pd
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag

from google import genai
from google.genai import types

# ============================================================
# Streamlit config
# ============================================================
st.set_page_config(page_title="ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ç‰ˆï¼‰", page_icon="ğŸ“–", layout="wide")
st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆå®Œå…¨ç‰ˆï¼‰")
st.markdown("""
**AI Ã— ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå®Œå…¨ç‰ˆï¼‰**  
ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ **è¨˜äº‹URLã®ã¿ã‚’å³å¯†ã«æŠ½å‡º** â†’ è¨˜äº‹æœ¬æ–‡ã‚’ **ãƒã‚¤ã‚ºé™¤å»ã—ã¦AIæŠ½å‡º** â†’ é‡è¤‡é™¤å¤–ã—ã¦ä¸€è¦§åŒ–ã—ã¾ã™ã€‚  
**è¿½åŠ æ©Ÿèƒ½:** è¨˜äº‹ã® **ãƒªãƒªãƒ¼ã‚¹æ—¥ï¼ˆå…¬é–‹æ—¥ï¼‰**ã€ã‚¤ãƒ™ãƒ³ãƒˆã® **ä½æ‰€ / ç·¯åº¦ / çµŒåº¦ï¼ˆå–ã‚ŒãŸã‚‰ï¼‰** ã‚’åé›†ã—ã¾ã™ã€‚
""")

# ============================================================
# Site rules
# ============================================================
@dataclass(frozen=True)
class SiteRule:
    name: str
    match_netloc: str
    article_path_allow: re.Pattern
    listing_next_hint_tokens: Tuple[str, ...] = ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "NEXT", "More", "MORE")
    # listingã«æ··ã–ã‚ŠãŒã¡ãªä¸è¦ãƒ‘ã‚¹
    deny_path_prefixes: Tuple[str, ...] = ("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account")
    # æœ¬æ–‡æŠ½å‡ºã®å„ªå…ˆã‚»ãƒ¬ã‚¯ã‚¿
    content_selectors: Tuple[str, ...] = ("article", "main", "div.article", "div#main", "div.content")

SITE_RULES: List[SiteRule] = [
    SiteRule(
        name="PRTIMES",
        match_netloc="prtimes.jp",
        article_path_allow=re.compile(r"^/main/html/rd/p/"),
        deny_path_prefixes=(
            "/ranking", "/company", "/categories", "/category", "/tag", "/tags",
            "/gourmet", "/entertainment", "/fashion", "/beauty", "/sports", "/technology", "/topics",
        ),
        content_selectors=("article", "main", "div.main-contents", "div#main", "div.body", "div.content")
    ),
    SiteRule(
        name="AtPress",
        match_netloc="atpress.ne.jp",
        article_path_allow=re.compile(r"^/news/\d+"),
        deny_path_prefixes=("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account"),
        content_selectors=("article", "main", "div#main", "div.newsDetail", "div.content")
    ),
]

def get_site_rule(url: str) -> Optional[SiteRule]:
    netloc = urllib.parse.urlparse(url).netloc.lower()
    for rule in SITE_RULES:
        if rule.match_netloc in netloc:
            return rule
    return None

# ============================================================
# Utils
# ============================================================
def normalize_date(text: str) -> str:
    """YYYYå¹´MMæœˆDDæ—¥ / YYYY/MM/DD ã®ã‚¼ãƒ­åŸ‹ã‚ç­‰ã€‚ISOã‚„æ™‚åˆ»ã‚’å«ã‚€å ´åˆã‚‚è»½ãæ•´å½¢ã€‚"""
    if not text or not isinstance(text, str):
        return ""
    t = text.strip()

    # ISOã£ã½ã„å ´åˆï¼ˆ2025-01-02T...ï¼‰ã¯æ—¥ä»˜éƒ¨åˆ†ã ã‘æ‹¾ã†
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", t)
    if m:
        y, mo, d = m.group(1), m.group(2).zfill(2), m.group(3).zfill(2)
        # å…ƒãŒã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã‚‰ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«å¯„ã›ã‚‹
        if "/" in t:
            return f"{y}/{mo}/{d}"
        return f"{y}å¹´{mo}æœˆ{d}æ—¥"

    def rep_ymd(m2):
        return f"{m2.group(1)}å¹´{m2.group(2).zfill(2)}æœˆ{m2.group(3).zfill(2)}æ—¥"

    t = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", rep_ymd, t)
    t = re.sub(r"(\d{4})/(\d{1,2})/(\d{1,2})", lambda m2: f"{m2.group(1)}/{m2.group(2).zfill(2)}/{m2.group(3).zfill(2)}", t)
    return t.strip()

def normalize_string(text) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace(" ", "").replace("ã€€", "")
    t = t.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return t.lower().strip()

def safe_json_parse(json_str: str) -> List[Dict]:
    if not json_str or not isinstance(json_str, str):
        return []
    s = json_str.replace("```json", "").replace("```", "").strip()

    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, list) else []
        except Exception:
            pass

    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return [obj] if isinstance(obj, dict) else []
        except Exception:
            pass

    return []

def is_valid_href(href: str) -> bool:
    if not href:
        return False
    h = href.strip()
    if h.startswith("#"):
        return False
    if h.lower().startswith("javascript:"):
        return False
    return True

def same_domain(url_a: str, url_b: str) -> bool:
    try:
        return urllib.parse.urlparse(url_a).netloc == urllib.parse.urlparse(url_b).netloc
    except Exception:
        return False

def fetch_html(session: requests.Session, url: str, timeout=(5, 20), max_retries=2) -> Optional[str]:
    for attempt in range(max_retries + 1):
        try:
            r = session.get(url, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            if r.status_code in (429, 503) and attempt < max_retries:
                time.sleep(1.2 * (attempt + 1))
                continue
            return None
        except requests.RequestException:
            if attempt < max_retries:
                time.sleep(1.0 * (attempt + 1))
                continue
            return None
    return None

def clean_soup(soup: BeautifulSoup) -> None:
    # ç¢ºå®Ÿã«æ¶ˆã—ãŸã„ã‚¿ã‚°
    for t in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
        try:
            t.decompose()
        except Exception:
            pass

    exclude_tokens = ["sidebar", "ranking", "recommend", "widget", "ad", "bread", "breadcrumb", "banner"]

    # find_all(True)ã§å…¨tagã€‚å£Šã‚Œè¦ç´ è€æ€§ã‚’ã¤ã‘ã‚‹
    for t in soup.find_all(True):
        if not isinstance(t, Tag):
            continue
        attrs = getattr(t, "attrs", None)
        if not isinstance(attrs, dict):
            continue

        cls_list = attrs.get("class") or []
        if not isinstance(cls_list, (list, tuple)):
            cls_list = [str(cls_list)]
        cls = " ".join(map(str, cls_list)).lower()

        if any(tok in cls for tok in exclude_tokens):
            try:
                t.decompose()
            except Exception:
                pass

def extract_main_text(soup: BeautifulSoup, rule: Optional[SiteRule]) -> str:
    """æœ¬æ–‡ã‚’(ã§ãã‚Œã°)main/articleã‹ã‚‰æŠ½å‡ºã€ã ã‚ãªã‚‰å…¨éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆ"""
    if rule:
        for sel in rule.content_selectors:
            try:
                node = soup.select_one(sel)
                if node:
                    return node.get_text("\n", strip=True)
            except Exception:
                continue
    return soup.get_text("\n", strip=True)

def split_text_into_chunks(text: str, chunk_size=8000, overlap=400):
    if not text:
        return
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        yield text[start:end]
        start = max(end - overlap, end)

def find_next_page_url(soup: BeautifulSoup, current_url: str, rule: Optional[SiteRule]) -> Optional[str]:
    # 1) rel=next
    link_next = soup.find("link", rel="next")
    if link_next and link_next.get("href") and is_valid_href(link_next["href"]):
        joined = urllib.parse.urljoin(current_url, link_next["href"])
        if same_domain(joined, current_url):
            return joined

    # 2) a[rel=next]
    a_next = soup.find("a", rel=lambda v: v and "next" in str(v).lower(), href=True)
    if a_next and is_valid_href(a_next["href"]):
        joined = urllib.parse.urljoin(current_url, a_next["href"])
        if same_domain(joined, current_url):
            return joined

    # 3) ãƒ†ã‚­ã‚¹ãƒˆãƒ’ãƒ³ãƒˆ
    tokens = rule.listing_next_hint_tokens if rule else ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "More")
    for a in soup.find_all("a", href=True):
        try:
            txt = a.get_text(strip=True)
        except Exception:
            continue
        if any(t in txt for t in tokens):
            href = a.get("href")
            if href and is_valid_href(href):
                joined = urllib.parse.urljoin(current_url, href)
                if same_domain(joined, current_url):
                    return joined
    return None

def is_article_url(url: str, rule: Optional[SiteRule]) -> bool:
    if not rule:
        return True  # unknown site: allow (æ±ç”¨é‹ç”¨)
    pu = urllib.parse.urlparse(url)
    path = pu.path or ""
    low = path.lower()

    if any(low.startswith(p) for p in rule.deny_path_prefixes):
        return False

    return bool(rule.article_path_allow.search(path))

def extract_article_links_from_listing(
    soup: BeautifulSoup,
    current_url: str,
    rule: Optional[SiteRule],
    link_limit: int = 80
) -> List[str]:
    """ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã®ã¿å³å¯†æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆãƒ«ãƒ¼ãƒ«é©ç”¨ï¼‰"""
    base = urllib.parse.urlparse(current_url)
    out: List[str] = []
    seen: Set[str] = set()

    for a in soup.find_all("a", href=True):
        href = a.get("href")
        if not is_valid_href(href):
            continue
        url = urllib.parse.urljoin(current_url, href)
        pu = urllib.parse.urlparse(url)

        if pu.netloc != base.netloc:
            continue

        # æœ€çµ‚ã‚²ãƒ¼ãƒˆï¼šè¨˜äº‹URLåˆ¤å®š
        if not is_article_url(url, rule):
            continue

        if url not in seen:
            seen.add(url)
            out.append(url)
        if len(out) >= link_limit:
            break

    return out

# ------------------------------------------------------------
# NEW: release date & JSON-LD location extraction
# ------------------------------------------------------------
def extract_release_date(soup: BeautifulSoup) -> str:
    """è¨˜äº‹ã®å…¬é–‹æ—¥(ãƒªãƒªãƒ¼ã‚¹æ—¥)ã‚’ meta/time ã‹ã‚‰æ‹¾ã†ã€‚å–ã‚Œãªã‘ã‚Œã°ç©ºã€‚"""
    meta_selectors = [
        ("meta", {"property": "article:published_time"}),
        ("meta", {"property": "og:published_time"}),
        ("meta", {"name": "pubdate"}),
        ("meta", {"name": "publishdate"}),
        ("meta", {"name": "date"}),
        ("meta", {"name": "dc.date"}),
        ("meta", {"name": "DC.date"}),
        ("meta", {"itemprop": "datePublished"}),
    ]
    for tag_name, attrs in meta_selectors:
        m = soup.find(tag_name, attrs=attrs)
        if m and m.get("content"):
            return normalize_date(str(m["content"]))

    # timeã‚¿ã‚°
    t = soup.find("time")
    if t:
        dt = t.get("datetime")
        if dt:
            return normalize_date(str(dt))
        txt = t.get_text(strip=True)
        if txt:
            return normalize_date(txt)

    return ""

def _as_list(x: Any) -> List[Any]:
    if x is None:
        return []
    return x if isinstance(x, list) else [x]

def extract_location_from_jsonld(soup: BeautifulSoup) -> Dict[str, str]:
    """
    schema.org Event / Place / PostalAddress / GeoCoordinates ã‹ã‚‰ä½æ‰€/ç·¯åº¦çµŒåº¦ã‚’æ‹¾ã†ã€‚
    è¿”ã‚Šå€¤: {"address": "", "latitude": "", "longitude": ""}
    """
    out = {"address": "", "latitude": "", "longitude": ""}

    for sc in soup.find_all("script", attrs={"type": "application/ld+json"}):
        raw = None
        try:
            raw = sc.string or sc.get_text(strip=True)
            if not raw:
                continue
            obj = json.loads(raw)
        except Exception:
            continue

        # JSON-LDã¯ dict or list or @graph ãŒã‚ã‚Šå¾—ã‚‹
        nodes: List[Any] = []
        for node in _as_list(obj):
            if isinstance(node, dict) and "@graph" in node:
                nodes.extend(_as_list(node.get("@graph")))
            else:
                nodes.append(node)

        for n in nodes:
            if not isinstance(n, dict):
                continue

            # Eventã£ã½ã„ location
            loc = n.get("location") or n.get("Place") or n.get("place")
            for loc_node in _as_list(loc):
                if not isinstance(loc_node, dict):
                    continue

                # address
                addr = loc_node.get("address")
                if isinstance(addr, dict):
                    parts = [
                        addr.get("addressRegion"),
                        addr.get("addressLocality"),
                        addr.get("streetAddress"),
                        addr.get("postalCode"),
                        addr.get("addressCountry"),
                    ]
                    addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                    if addr_text and not out["address"]:
                        out["address"] = addr_text
                elif isinstance(addr, str) and addr.strip() and not out["address"]:
                    out["address"] = addr.strip()

                # geo
                geo = loc_node.get("geo")
                if isinstance(geo, dict):
                    lat = geo.get("latitude")
                    lon = geo.get("longitude")
                    if lat is not None and not out["latitude"]:
                        out["latitude"] = str(lat).strip()
                    if lon is not None and not out["longitude"]:
                        out["longitude"] = str(lon).strip()

            # Eventç›´ä¸‹ã® address/geo ãŒã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            addr2 = n.get("address")
            if isinstance(addr2, dict) and not out["address"]:
                parts = [
                    addr2.get("addressRegion"),
                    addr2.get("addressLocality"),
                    addr2.get("streetAddress"),
                    addr2.get("postalCode"),
                ]
                addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                if addr_text:
                    out["address"] = addr_text

            geo2 = n.get("geo")
            if isinstance(geo2, dict):
                lat = geo2.get("latitude")
                lon = geo2.get("longitude")
                if lat is not None and not out["latitude"]:
                    out["latitude"] = str(lat).strip()
                if lon is not None and not out["longitude"]:
                    out["longitude"] = str(lon).strip()

            if out["address"] or out["latitude"] or out["longitude"]:
                return out  # å–ã‚ŒãŸã‚‰æ—©æœŸreturn

    return out

# ------------------------------------------------------------
# Gemini extraction
# ------------------------------------------------------------
def ai_extract_events_from_text(
    client: genai.Client,
    model_name: str,
    temperature: float,
    text: str,
    today: datetime.date,
) -> List[Dict]:
    all_items: List[Dict] = []
    for chunk in split_text_into_chunks(text, chunk_size=8000, overlap=400):
        if not chunk or len(chunk) < 120:
            continue

        prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’JSONé…åˆ—ã§æ¼ã‚ŒãªãæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã€ç¾åœ¨æ—¥ä»˜: {today}ã€‘

[æŠ½å‡ºãƒ«ãƒ¼ãƒ«]
- æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå±•ç¤ºã€å‚¬äº‹ã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€å‹Ÿé›†ã€ç™ºè¡¨ä¼šã€ã‚»ãƒŸãƒŠãƒ¼ç­‰ï¼‰ã‚„ã€æ—¥æ™‚ãƒ»æœŸé–“ãƒ»å ´æ‰€ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å¯èƒ½ãªé™ã‚ŠæŠ½å‡ºã€‚
- çœç•¥å³ç¦ã€‚ãŸã ã—ã€Œä¼æ¥­ãƒ•ãƒƒã‚¿ãƒ»å•ã„åˆã‚ã›å…ˆãƒ†ãƒ³ãƒ—ãƒ¬ã€ãªã©ã®éã‚¤ãƒ™ãƒ³ãƒˆå®šå‹æ–‡ã¯ç„¡ç†ã«æ‹¾ã‚ãªã„ã€‚
- date_info ã¯æœ¬æ–‡ã®è¡¨è¨˜ã®ã¾ã¾ã§ã‚‚è‰¯ã„ãŒã€å¯èƒ½ãªã‚‰ YYYYå¹´MMæœˆDDæ—¥ / YYYY/MM/DD / æœŸé–“è¡¨ç¾ï¼ˆä¾‹: 2025å¹´01æœˆ01æ—¥ã€œ2025å¹´02æœˆ01æ—¥ï¼‰ã€‚
- address / latitude / longitude ã¯æœ¬æ–‡ã‹ã‚‰æ¨å®šã§ãã‚‹ç¯„å›²ã§ã‚ˆã„ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰ã€‚
- å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ï¼ˆèª¬æ˜æ–‡ã¯ç¦æ­¢ï¼‰ã€‚

[JSONå½¢å¼]
[
  {{
    "name": "ã‚¿ã‚¤ãƒˆãƒ«",
    "place": "å ´æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "address": "ä½æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "latitude": "ç·¯åº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "longitude": "çµŒåº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "date_info": "æ—¥ä»˜ã‚„æœŸé–“ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "description": "æ¦‚è¦ï¼ˆçŸ­ã‚ã«ï¼‰"
  }}
]

æœ¬æ–‡:
{chunk}
"""
        try:
            res = client.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=float(temperature)
                )
            )
            extracted = safe_json_parse(res.text)
            if isinstance(extracted, list):
                for item in extracted:
                    if not item or not isinstance(item, dict):
                        continue
                    name = str(item.get("name") or "").strip()
                    if not name:
                        continue
                    out = {
                        "name": name,
                        "place": str(item.get("place") or "").strip(),
                        "address": str(item.get("address") or "").strip(),
                        "latitude": str(item.get("latitude") or "").strip(),
                        "longitude": str(item.get("longitude") or "").strip(),
                        "date_info": normalize_date(str(item.get("date_info") or "").strip()),
                        "description": str(item.get("description") or "").strip(),
                    }
                    all_items.append(out)
        except Exception:
            continue

    return all_items

# ============================================================
# Sidebar UI
# ============================================================
with st.sidebar:
    st.header("1. å¯¾è±¡ã‚µã‚¤ãƒˆ")

    PRESET_URLS = {
        "PRTIMES (ã‚°ãƒ«ãƒ¡)": "https://prtimes.jp/gourmet/",
        "PRTIMES (ã‚¨ãƒ³ã‚¿ãƒ¡)": "https://prtimes.jp/entertainment/",
        "AtPress (ã‚°ãƒ«ãƒ¡)": "https://www.atpress.ne.jp/news/food",
        "AtPress (æ–°ç€)": "https://www.atpress.ne.jp/news",
    }

    selected_presets = st.multiselect(
        "ãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (ã‚°ãƒ«ãƒ¡)"],
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URLï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ä¸€è¦§URLæ¨å¥¨ï¼‰")
    custom_urls_text = st.text_area("URLï¼ˆ1è¡Œã«1ã¤ï¼‰", height=110)

    st.divider()
    st.header("2. æ¢ç´¢è¨­å®š")
    max_pages = st.slider("ä¸€è¦§ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒšãƒ¼ã‚¸é€ã‚Šå›æ•°ï¼‰", 1, 30, 6)
    link_limit_per_page = st.slider("1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šåé›†ã™ã‚‹è¨˜äº‹URLä¸Šé™", 10, 300, 80, step=10)
    max_articles_total = st.slider("ç·è¨˜äº‹æ•°ã®ä¸Šé™ï¼ˆå®‰å…¨ç­–ï¼‰", 20, 2000, 400, step=20)
    sleep_sec = st.slider("ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ï¼ˆç§’ï¼‰", 0.0, 2.0, 0.5, step=0.1)

    st.divider()
    st.header("3. Geminiè¨­å®š")
    model_name = st.text_input("ãƒ¢ãƒ‡ãƒ«å", value="gemini-2.0-flash")
    temperature = st.slider("temperatureï¼ˆ0æ¨å¥¨ï¼‰", 0.0, 1.0, 0.0, step=0.1)

    st.divider()
    st.header("4. æ—¢å­˜CSVã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–")
    uploaded_file = st.file_uploader("éå»CSVï¼ˆé‡è¤‡é™¤å¤–ç”¨ï¼‰", type="csv")

# ============================================================
# Load existing fingerprints
# ============================================================
existing_fingerprints: Set[Tuple[str, str]] = set()
if uploaded_file is not None:
    try:
        existing_df = pd.read_csv(uploaded_file)
        name_col = next((c for c in existing_df.columns if "ã‚¤ãƒ™ãƒ³ãƒˆå" in c or c.lower() in ["name", "title"]), None)
        place_col = next((c for c in existing_df.columns if "å ´æ‰€" in c or c.lower() in ["place", "location"]), None)

        if name_col:
            for _, row in existing_df.iterrows():
                n = normalize_string(row.get(name_col, ""))
                p = normalize_string(row.get(place_col, "")) if place_col else ""
                if n:
                    existing_fingerprints.add((n, p))
            st.sidebar.success(f"ğŸ“š {len(existing_fingerprints)}ä»¶ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
        else:
            st.sidebar.warning("CSVã«ã‚¤ãƒ™ãƒ³ãƒˆååˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡é™¤å¤–ãªã—ã§ç¶šè¡Œï¼‰ã€‚")
    except Exception as e:
        st.sidebar.error(f"CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# Session state
# ============================================================
if "extracted_data" not in st.session_state:
    st.session_state.extracted_data = None
if "last_update" not in st.session_state:
    st.session_state.last_update = None

# ============================================================
# Main
# ============================================================
if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    # API key
    api_key = None
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except Exception:
        api_key = os.environ.get("GOOGLE_API_KEY")

    if not api_key:
        st.error("âš ï¸ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆst.secrets ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ï¼‰ã€‚")
        st.stop()

    # targets
    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})

    if custom_urls_text:
        for u in custom_urls_text.splitlines():
            u = u.strip()
            if u.startswith("http"):
                domain = urllib.parse.urlparse(u).netloc
                targets.append({"url": u, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})

    unique_targets = {t["url"]: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    today = datetime.date.today()
    client = genai.Client(api_key=api_key)

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    })

    status = st.empty()
    progress = st.progress(0.0)

    # --------------------------------------------------------
    # 1) Collect article URLs from listings
    # --------------------------------------------------------
    collected: List[Tuple[str, str]] = []  # (url, source_label)
    collected_seen: Set[str] = set()
    visited_listing: Set[str] = set()

    total_units = max(len(targets) * max_pages, 1)
    done_units = 0

    for target in targets:
        base_url = target["url"]
        label = target["label"]
        current_url = base_url
        rule = get_site_rule(current_url)

        for page_num in range(1, max_pages + 1):
            done_units += 1
            progress.progress(min(done_units / total_units, 1.0))

            if current_url in visited_listing:
                status.warning(f"ğŸ” ä¸€è¦§URLå†è¨ªã®ãŸã‚åœæ­¢: {current_url}")
                break
            visited_listing.add(current_url)

            status.info(f"ğŸ“„ ä¸€è¦§å–å¾—: {label} | {page_num}/{max_pages}\n{current_url}")

            html = fetch_html(session, current_url)
            if not html:
                status.warning(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {current_url}")
                break

            soup = BeautifulSoup(html, "html.parser")

            # æ¬¡ãƒšãƒ¼ã‚¸
            next_url = find_next_page_url(soup, current_url, rule)

            # è¨˜äº‹URLæŠ½å‡ºï¼ˆå³å¯†ï¼‰
            links = extract_article_links_from_listing(soup, current_url, rule, link_limit=link_limit_per_page)

            # åé›†
            add_count = 0
            for u in links:
                if u not in collected_seen:
                    collected_seen.add(u)
                    collected.append((u, label))
                    add_count += 1

            status.info(f"ğŸ”— è¨˜äº‹URLåé›†: +{add_count}ä»¶ï¼ˆç´¯è¨ˆ {len(collected)}ä»¶ï¼‰")

            if len(collected) >= max_articles_total:
                break

            if not next_url:
                break

            current_url = next_url
            time.sleep(sleep_sec)

        if len(collected) >= max_articles_total:
            break

    collected = collected[:max_articles_total]

    if not collected:
        progress.empty()
        status.error("ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.session_state.extracted_data = None
        st.stop()

    # --------------------------------------------------------
    # 2) Extract events from article pages
    # --------------------------------------------------------
    status.info(f"ğŸ§  è¨˜äº‹ãƒšãƒ¼ã‚¸è§£æé–‹å§‹ï¼ˆç· {len(collected)} ä»¶ï¼‰")
    extracted_all: List[Dict] = []

    # é‡è¤‡é™¤å¤–ã‚’é«˜é€ŸåŒ–
    run_fingerprints: Set[Tuple[str, str]] = set()  # (name_norm, place_norm)

    skipped_duplicate_csv = 0
    skipped_duplicate_run = 0
    failed_articles = 0
    non_article_skipped = 0

    for i, (article_url, label) in enumerate(collected, start=1):
        progress.progress(min(i / max(len(collected), 1), 1.0))
        status.info(f"ğŸ§  è¨˜äº‹è§£æ {i}/{len(collected)}: {article_url}")

        rule = get_site_rule(article_url)

        # æœ€çµ‚ã‚²ãƒ¼ãƒˆï¼šè¨˜äº‹URLã§ãªã‘ã‚Œã°è§£æã—ãªã„
        if not is_article_url(article_url, rule):
            non_article_skipped += 1
            continue

        html = fetch_html(session, article_url)
        if not html:
            failed_articles += 1
            continue

        soup = BeautifulSoup(html, "html.parser")

        # NEW: ãƒªãƒªãƒ¼ã‚¹æ—¥ãƒ»JSON-LD ä½ç½®æƒ…å ±
        release_date = extract_release_date(soup)
        loc = extract_location_from_jsonld(soup)  # {"address","latitude","longitude"}

        clean_soup(soup)
        text = extract_main_text(soup, rule)

        # AIæŠ½å‡º
        items = ai_extract_events_from_text(client, model_name, temperature, text, today)

        for item in items:
            # NEW: è¨˜äº‹å˜ä½æƒ…å ±ã‚’ä»˜ä¸
            item["release_date"] = release_date

            # JSON-LDã§å–ã‚ŒãŸå€¤ã‚’å„ªå…ˆï¼ˆGeminiãŒç©ºãªã‚‰è£œå®Œï¼‰
            if loc.get("address") and not item.get("address"):
                item["address"] = loc["address"]
            if loc.get("latitude") and not item.get("latitude"):
                item["latitude"] = loc["latitude"]
            if loc.get("longitude") and not item.get("longitude"):
                item["longitude"] = loc["longitude"]

            n = normalize_string(item.get("name", ""))
            p = normalize_string(item.get("place", ""))

            if not n:
                continue

            fp = (n, p)

            if fp in existing_fingerprints:
                skipped_duplicate_csv += 1
                continue

            if fp in run_fingerprints:
                skipped_duplicate_run += 1
                continue

            run_fingerprints.add(fp)

            item["source_label"] = label
            item["source_url"] = article_url
            extracted_all.append(item)

        time.sleep(sleep_sec)

    progress.empty()

    if not extracted_all:
        status.warning(
            f"æŠ½å‡ºçµæœãŒ0ä»¶ã§ã—ãŸã€‚\n"
            f"- è¨˜äº‹å¤±æ•—: {failed_articles}ä»¶\n"
            f"- éè¨˜äº‹URLã‚¹ã‚­ãƒƒãƒ—: {non_article_skipped}ä»¶\n"
            f"- CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶"
        )
        st.session_state.extracted_data = None
        st.stop()

    st.session_state.extracted_data = extracted_all
    st.session_state.last_update = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    status.success(
        f"ğŸ‰ å®Œäº†ï¼æ–°è¦ {len(extracted_all)} ä»¶\n"
        f"- CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶\n"
        f"- ä»Šå›é‡è¤‡é™¤å¤–: {skipped_duplicate_run}ä»¶\n"
        f"- éè¨˜äº‹URLã‚¹ã‚­ãƒƒãƒ—: {non_article_skipped}ä»¶\n"
        f"- è¨˜äº‹å¤±æ•—: {failed_articles}ä»¶"
    )

# ============================================================
# Result rendering
# ============================================================
if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)

    st.markdown(f"**å–å¾—ä»¶æ•°: {len(df)}**ï¼ˆæ›´æ–°: {st.session_state.last_update}ï¼‰")

    display_df = df.rename(columns={
        "release_date": "ãƒªãƒªãƒ¼ã‚¹æ—¥",
        "date_info": "æœŸé–“",
        "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
        "place": "å ´æ‰€",
        "address": "ä½æ‰€",
        "latitude": "ç·¯åº¦",
        "longitude": "çµŒåº¦",
        "description": "æ¦‚è¦",
        "source_label": "æƒ…å ±æº",
        "source_url": "URL"
    })

    desired_cols = ["ãƒªãƒªãƒ¼ã‚¹æ—¥", "æœŸé–“", "ã‚¤ãƒ™ãƒ³ãƒˆå", "å ´æ‰€", "ä½æ‰€", "ç·¯åº¦", "çµŒåº¦", "æ¦‚è¦", "æƒ…å ±æº", "URL"]
    cols = [c for c in desired_cols if c in display_df.columns]
    display_df = display_df[cols]

    # æœŸé–“ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–‡å­—åˆ—ãªã®ã§ç°¡æ˜“ã€‚å³å¯†åŒ–ã¯åˆ¥é€”ï¼‰
    if "æœŸé–“" in display_df.columns:
        try:
            display_df = display_df.sort_values("æœŸé–“", na_position="last")
        except Exception:
            pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— Link"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large"),
        },
        hide_index=True
    )

    csv_bytes = display_df.to_csv(index=False).encode("utf-8_sig")
    st.download_button("ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_bytes, "events_full.csv", "text/csv")
