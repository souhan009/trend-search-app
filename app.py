import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import urllib.parse
import re

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢ï¼ˆå¤šãƒšãƒ¼ã‚¸å¯¾å¿œï¼‰", page_icon="ğŸ“–", layout="wide")

st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒª")
st.markdown("""
**AI Ã— ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°**
Webãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã€**ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ã‚„ã€Œæ¬¡ã¸ã€ã®ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•ã§è¾¿ã£ã¦**ã€å¥¥ã«ã‚ã‚‹è¨˜äº‹ã¾ã§æŠ½å‡ºã—ã¾ã™ã€‚
""")

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def normalize_date(text):
    """æ—¥ä»˜ã‚’ã‚¼ãƒ­åŸ‹ã‚YYYYå¹´MMæœˆDDæ—¥å½¢å¼ã«çµ±ä¸€"""
    if not text: return text
    def replace_func(match):
        return f"{match.group(1)}å¹´{match.group(2).zfill(2)}æœˆ{match.group(3).zfill(2)}æ—¥"
    text = re.sub(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', replace_func, text)
    text = re.sub(r'(\d{4})/(\d{1,2})/(\d{1,2})', lambda m: f"{m.group(1)}/{m.group(2).zfill(2)}/{m.group(3).zfill(2)}", text)
    return text

def normalize_string(text):
    """æ–‡å­—åˆ—æ¯”è¼ƒç”¨ã®æ­£è¦åŒ–é–¢æ•°"""
    if not isinstance(text, str):
        return ""
    text = text.replace(" ", "").replace("ã€€", "")
    text = text.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return text.lower()

def safe_json_parse(json_str):
    """ä¸å®Œå…¨ãªJSONæ–‡å­—åˆ—ã‹ã‚‰ã€æœ‰åŠ¹ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’æ•‘å‡ºã™ã‚‹"""
    if not json_str: return []
    json_str = json_str.replace("```json", "").replace("```", "").strip()
    
    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        try:
            last_brace_index = json_str.rfind("}")
            if last_brace_index == -1: return [] 
            repaired_json = json_str[:last_brace_index+1] + "]"
            return json.loads(repaired_json)
        except:
            return []

def split_text_into_chunks(text, chunk_size=8000, overlap=500):
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
    if not text: return
    start = 0
    text_len = len(text)
    while start < text_len:
        end = start + chunk_size
        yield text[start:end]
        start = end - overlap

def find_next_page_url(soup, current_url):
    """
    HTMLå†…ã‹ã‚‰ã€Œæ¬¡ã¸ã€ã‚„ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ã®URLã‚’æ¢ã—å‡ºã™é–¢æ•°
    """
    next_url = None
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ç‰¹å®šã‚¯ãƒ©ã‚¹ï¼ˆå„ªå…ˆï¼‰
    target_btn = soup.select_one("a.js-list-article-more-button")
    if target_btn and target_btn.get('href'):
        next_url = target_btn['href']
        
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: rel="next"
    if not next_url:
        link_next = soup.find("link", rel="next")
        if link_next and link_next.get('href'):
            next_url = link_next['href']

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒ©ã‚¹
    if not next_url:
        # "æ¬¡ã¸", "Next", "More" ã‚’å«ã‚€aã‚¿ã‚°ã€ã¾ãŸã¯ page-link ãªã©ã®ã‚¯ãƒ©ã‚¹
        candidates = soup.find_all("a", href=True)
        for a in candidates:
            text = a.get_text(strip=True)
            cls = " ".join(a.get("class", []))
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚„ã‚¯ãƒ©ã‚¹åã§åˆ¤å®š
            if "æ¬¡ã¸" in text or "Next" in text or "more" in cls.lower() or "next" in cls.lower():
                # æ˜ã‚‰ã‹ã«ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹ã‚ˆã†ãªãƒªãƒ³ã‚¯ã¯é™¤å¤–
                if len(a['href']) > 2: 
                    next_url = a['href']
                    break
    
    if next_url:
        # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        return urllib.parse.urljoin(current_url, next_url)
    
    return None

# --- Session State ---
if 'extracted_data' not in st.session_state:
    st.session_state.extracted_data = None
if 'last_update' not in st.session_state:
    st.session_state.last_update = None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("1. èª­ã¿è¾¼ã¿å¯¾è±¡")
    
    PRESET_URLS = {
        "PRTIMES (ã‚°ãƒ«ãƒ¡)": "https://prtimes.jp/gourmet/",
        "PRTIMES (ã‚¨ãƒ³ã‚¿ãƒ¡)": "https://prtimes.jp/entertainment/",
        "AtPress (ã‚°ãƒ«ãƒ¡)": "https://www.atpress.ne.jp/news/food",
        "AtPress (æ–°ç€)": "https://www.atpress.ne.jp/news",
    }
    
    selected_presets = st.multiselect(
        "ã‚µã‚¤ãƒˆã‚’é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (ã‚°ãƒ«ãƒ¡)"]
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area("URLã‚’å…¥åŠ› (1è¡Œã«1ã¤)", height=100)
    
    st.markdown("---")
    st.header("2. æ¢ç´¢æ·±åº¦")
    max_pages = st.slider("èª­ã¿è¾¼ã‚€æœ€å¤§ãƒšãƒ¼ã‚¸æ•°", 1, 10, 3, help="ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ã‚’ä½•å›è¾¿ã‚‹ã‹æŒ‡å®šã—ã¾ã™ã€‚å¤šã„ã¨æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚")
    
    st.markdown("---")
    st.markdown("### 3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿é™¤å¤–")
    uploaded_file = st.file_uploader("éå»CSV (é‡è¤‡é™¤å¤–ç”¨)", type="csv")
    
    existing_fingerprints = set()
    if uploaded_file is not None:
        try:
            existing_df = pd.read_csv(uploaded_file)
            count = 0
            name_col = next((col for col in existing_df.columns if 'ã‚¤ãƒ™ãƒ³ãƒˆå' in col or 'Name' in col), None)
            place_col = next((col for col in existing_df.columns if 'å ´æ‰€' in col or 'Place' in col), None)

            if name_col:
                for _, row in existing_df.iterrows():
                    n = normalize_string(row[name_col])
                    p = normalize_string(row[place_col]) if place_col else ""
                    existing_fingerprints.add((n, p))
                    count += 1
                st.success(f"ğŸ“š {count}ä»¶ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
        except Exception as e:
            st.error(f"CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆä½œæˆ
    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})
    
    if custom_urls_text:
        for url in custom_urls_text.split('\n'):
            url = url.strip()
            if url and url.startswith("http"):
                domain = urllib.parse.urlparse(url).netloc
                targets.append({"url": url, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})
    
    # é‡è¤‡URLå‰Šé™¤
    unique_targets = {t['url']: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    all_data = []
    client = genai.Client(api_key=api_key)
    today = datetime.date.today()
    
    main_progress = st.progress(0)
    status_text = st.empty()
    skipped_count_duplicate_csv = 0
    
    # --- ã‚µã‚¤ãƒˆã”ã¨ã®ãƒ«ãƒ¼ãƒ— ---
    for idx, target in enumerate(targets):
        base_url = target['url']
        label = target['label']
        
        current_url = base_url
        
        # --- ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ«ãƒ¼ãƒ— (æŒ‡å®šå›æ•°ã¾ã§) ---
        for page_num in range(1, max_pages + 1):
            
            progress_percent = (idx / len(targets)) + ((page_num / max_pages) / len(targets))
            main_progress.progress(min(progress_percent, 1.0))
            status_text.info(f"ğŸ” {label} | {page_num}ãƒšãƒ¼ã‚¸ç›®ã‚’è§£æä¸­...\nURL: {current_url}")
            
            try:
                headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124"}
                response = requests.get(current_url, headers=headers, timeout=15)
                response.encoding = response.apparent_encoding
                
                if response.status_code != 200:
                    st.warning(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {current_url}")
                    break

                soup = BeautifulSoup(response.text, "html.parser")
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã®URLã‚’æ¢ã—ã¦ãŠã
                next_page_url = find_next_page_url(soup, current_url)
                
                # --- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ---
                for tag in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
                    tag.decompose()
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä»¥å¤–ã‚’å‰Šé™¤ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
                exclude = ['sidebar', 'ranking', 'recommend', 'widget', 'ad', 'bread']
                for tag in soup.find_all(attrs={"class": True}):
                    if not tag: continue
                    c_str = str(tag.get("class")).lower()
                    if any(x in c_str for x in exclude):
                        tag.decompose()
                
                full_text = soup.get_text(separator="\n", strip=True)
                chunks = list(split_text_into_chunks(full_text))
                
                # --- AIæŠ½å‡º (ãƒãƒ£ãƒ³ã‚¯ã”ã¨) ---
                for chunk in chunks:
                    if not chunk: continue
                    
                    prompt = f"""
                    ä»¥ä¸‹ã®Webãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’JSONãƒªã‚¹ãƒˆã§æŠ½å‡ºã›ã‚ˆã€‚
                    ã€ç¾åœ¨: {today}ã€‘
                    
                    [å‡ºåŠ›ãƒ«ãƒ¼ãƒ«]
                    - ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ã‚‹æƒ…å ±ã¯å…¨ã¦æŠ½å‡ºã™ã‚‹ã“ã¨ã€‚çœç•¥å³ç¦ã€‚
                    - å¤ã„ã‚¤ãƒ™ãƒ³ãƒˆã‚‚æŠ½å‡ºã—ã¦ã‚ˆã„ã€‚
                    
                    Text:
                    {chunk[:10000]}
                    
                    JSON Output Example:
                    [
                        {{
                            "name": "ã‚¿ã‚¤ãƒˆãƒ«",
                            "place": "å ´æ‰€",
                            "date_info": "YYYYå¹´MMæœˆDDæ—¥",
                            "description": "æ¦‚è¦"
                        }}
                    ]
                    """
                    
                    try:
                        ai_res = client.models.generate_content(
                            model="gemini-2.0-flash-exp",
                            contents=prompt,
                            config=types.GenerateContentConfig(
                                response_mime_type="application/json", 
                                temperature=0.0
                            )
                        )
                        extracted = safe_json_parse(ai_res.text)
                        
                        if isinstance(extracted, list):
                            for item in extracted:
                                if not item.get('name'): continue
                                
                                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                n = normalize_string(item['name'])
                                p = normalize_string(item.get('place', ''))
                                
                                # CSVã¨ã®é‡è¤‡ç¢ºèª
                                if (n, p) in existing_fingerprints:
                                    skipped_count_duplicate_csv += 1
                                    continue
                                
                                item['source_label'] = label
                                item['source_url'] = current_url # ãƒšãƒ¼ã‚¸URLã‚’ä¿å­˜
                                item['date_info'] = normalize_date(item.get('date_info', ''))
                                all_data.append(item)
                                
                    except Exception as e:
                        print(f"AI Error: {e}")
                        time.sleep(1)
            
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒãªã‘ã‚Œã°çµ‚äº†ã€ã‚ã‚Œã°URLæ›´æ–°ã—ã¦ãƒ«ãƒ¼ãƒ—ç¶™ç¶š
                if not next_page_url:
                    break
                current_url = next_page_url
                time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                
            except Exception as e:
                st.warning(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                break

    main_progress.empty()

    # --- çµæœé›†è¨ˆ ---
    if not all_data:
        if skipped_count_duplicate_csv > 0:
            st.warning(f"å–å¾—ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦CSVå†…ã®æ—¢çŸ¥æƒ…å ±ã§ã—ãŸã€‚ï¼ˆé™¤å¤–: {skipped_count_duplicate_csv}ä»¶ï¼‰")
        else:
            st.error("æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.session_state.extracted_data = None
    else:
        # é‡è¤‡æ’é™¤ (ãƒšãƒ¼ã‚¸ã¾ãŸãç­‰)
        unique_data = []
        seen = set()
        for d in all_data:
            key = (normalize_string(d['name']), normalize_string(d.get('place','')))
            if key not in seen:
                seen.add(key)
                unique_data.append(d)
        
        st.session_state.extracted_data = unique_data
        st.session_state.last_update = datetime.datetime.now().strftime("%H:%M:%S")
        status_text.success(f"ğŸ‰ å®Œäº†ï¼ æ–°è¦ {len(unique_data)} ä»¶ (CSVé™¤å¤–: {skipped_count_duplicate_csv}ä»¶)")

# --- çµæœè¡¨ç¤º ---
if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)
    
    st.markdown(f"**å–å¾—ä»¶æ•°: {len(df)}**")
    
    # è¡¨ç¤ºç”¨åŠ å·¥
    display_df = df.rename(columns={
        'date_info': 'æœŸé–“', 'name': 'ã‚¤ãƒ™ãƒ³ãƒˆå', 
        'place': 'å ´æ‰€', 'description': 'æ¦‚è¦', 
        'source_label': 'æƒ…å ±æº', 'source_url': 'URL'
    })
    
    # æœŸé–“ã§ã‚½ãƒ¼ãƒˆ
    try:
        display_df = display_df.sort_values('æœŸé–“')
    except: pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— Link"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large")
        },
        hide_index=True
    )
    
    csv = display_df.to_csv(index=False).encode('utf-8_sig')
    st.download_button("ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv, "events_full.csv", "text/csv")
