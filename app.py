import streamlit as st
import datetime
import os
import json
import time
import re
import urllib.parse
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set, Any

import pandas as pd
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag

from google import genai
from google.genai import types

# ============================================================
# Streamlit config
# ============================================================
st.set_page_config(page_title="„Ç§„Éô„É≥„ÉàÊÉÖÂ†±ÊäΩÂá∫ÔºàÂÆâÂÆöÁâàÔºâ", page_icon="üìñ", layout="wide")
st.title("üìñ „Ç§„Éô„É≥„ÉàÊÉÖÂ†±ÊäΩÂá∫„Ç¢„Éó„É™ÔºàÂÆâÂÆöÁâàÔºâ")
st.markdown("""
**AI √ó „Çπ„Éû„Éº„Éà„ÇØ„É≠„Éº„É™„É≥„Ç∞ÔºàÂÆâÂÆöÁâàÔºâ** ‰∏ÄË¶ß„Éö„Éº„Ç∏„Åã„ÇâË®ò‰∫ãURL„ÇíÂé≥ÂØÜ„Å´ÊäΩÂá∫ ‚Üí Êú¨Êñá„ÇíAIËß£Êûê ‚Üí ÈáçË§áÈô§Â§ñ„Åó„Å¶‰∏ÄË¶ßÂåñ„ÄÇ  
‚Äª„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„ÇíÂê´„Åæ„Å™„ÅÑ„ÄÅ1‰ª∂„Åö„Å§Á¢∫ÂÆü„Å´Âá¶ÁêÜ„Åô„Çã„Éê„Éº„Ç∏„Éß„É≥„Åß„Åô„ÄÇ
""")

# ============================================================
# Site rules
# ============================================================
@dataclass(frozen=True)
class SiteRule:
    name: str
    match_netloc: str
    article_path_allow: re.Pattern
    listing_next_hint_tokens: Tuple[str, ...] = ("Ê¨°„Å∏", "Ê¨°„ÅÆ", "„ÇÇ„Å£„Å®Ë¶ã„Çã", "Next", "NEXT", "More", "MORE")
    deny_path_prefixes: Tuple[str, ...] = ("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account")
    content_selectors: Tuple[str, ...] = ("article", "main", "div.article", "div#main", "div.content")

SITE_RULES: List[SiteRule] = [
    SiteRule(
        name="PRTIMES",
        match_netloc="prtimes.jp",
        article_path_allow=re.compile(r"^/main/html/rd/p/"),
        deny_path_prefixes=(
            "/ranking", "/company", "/categories", "/category", "/tag", "/tags",
            "/gourmet", "/entertainment", "/fashion", "/beauty", "/sports", "/technology", "/topics",
        ),
        content_selectors=("article", "main", "div.main-contents", "div#main", "div.body", "div.content")
    ),
    SiteRule(
        name="AtPress",
        match_netloc="atpress.ne.jp",
        article_path_allow=re.compile(r"^/news/\d+"),
        deny_path_prefixes=("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account"),
        content_selectors=("article", "main", "div#main", "div.newsDetail", "div.content")
    ),
]

def get_site_rule(url: str) -> Optional[SiteRule]:
    netloc = urllib.parse.urlparse(url).netloc.lower()
    for rule in SITE_RULES:
        if rule.match_netloc in netloc:
            return rule
    return None

# ============================================================
# Utils
# ============================================================
def normalize_date(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    t = text.strip()
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", t)
    if m:
        y, mo, d = m.group(1), m.group(2).zfill(2), m.group(3).zfill(2)
        if "/" in t:
            return f"{y}/{mo}/{d}"
        return f"{y}Âπ¥{mo}Êúà{d}Êó•"
    def rep_ymd(m2):
        return f"{m2.group(1)}Âπ¥{m2.group(2).zfill(2)}Êúà{m2.group(3).zfill(2)}Êó•"
    t = re.sub(r"(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•", rep_ymd, t)
    t = re.sub(r"(\d{4})/(\d{1,2})/(\d{1,2})", lambda m2: f"{m2.group(1)}/{m2.group(2).zfill(2)}/{m2.group(3).zfill(2)}", t)
    return t.strip()

def normalize_string(text) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace(" ", "").replace("„ÄÄ", "")
    t = t.replace("Ôºà", "").replace("Ôºâ", "").replace("(", "").replace(")", "")
    return t.lower().strip()

def safe_json_parse(json_str: str) -> List[Dict]:
    if not json_str or not isinstance(json_str, str):
        return []
    s = json_str.replace("```json", "").replace("```", "").strip()
    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, list) else []
        except Exception:
            pass
    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return [obj] if isinstance(obj, dict) else []
        except Exception:
            pass
    return []

def is_valid_href(href: str) -> bool:
    if not href:
        return False
    h = href.strip()
    if h.startswith("#"):
        return False
    if h.lower().startswith("javascript:"):
        return False
    return True

def same_domain(url_a: str, url_b: str) -> bool:
    try:
        return urllib.parse.urlparse(url_a).netloc == urllib.parse.urlparse(url_b).netloc
    except Exception:
        return False

def fetch_html(session: requests.Session, url: str, timeout=(5, 20), max_retries=2) -> Optional[str]:
    for attempt in range(max_retries + 1):
        try:
            r = session.get(url, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            if r.status_code in (429, 503) and attempt < max_retries:
                time.sleep(1.2 * (attempt + 1))
                continue
            return None
        except requests.RequestException:
            if attempt < max_retries:
                time.sleep(1.0 * (attempt + 1))
                continue
            return None
    return None

def clean_soup(soup: BeautifulSoup) -> None:
    for t in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
        try:
            t.decompose()
        except Exception:
            pass
    exclude_tokens = ["sidebar", "ranking", "recommend", "widget", "ad", "bread", "breadcrumb", "banner"]
    for t in soup.find_all(True):
        if not isinstance(t, Tag):
            continue
        attrs = getattr(t, "attrs", None)
        if not isinstance(attrs, dict):
            continue
        cls_list = attrs.get("class") or []
        if not isinstance(cls_list, (list, tuple)):
            cls_list = [str(cls_list)]
        cls = " ".join(map(str, cls_list)).lower()
        if any(tok in cls for tok in exclude_tokens):
            try:
                t.decompose()
            except Exception:
                pass

def extract_main_text(soup: BeautifulSoup, rule: Optional[SiteRule]) -> str:
    if rule:
        for sel in rule.content_selectors:
            try:
                node = soup.select_one(sel)
                if node:
                    return node.get_text("\n", strip=True)
            except Exception:
                continue
    return soup.get_text("\n", strip=True)

def split_text_into_chunks(text: str, chunk_size=8000, overlap=400):
    if not text:
        return
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        yield text[start:end]
        start = max(end - overlap, end)

def find_next_page_url(soup: BeautifulSoup, current_url: str, rule: Optional[SiteRule]) -> Optional[str]:
    link_next = soup.find("link", rel="next")
    if link_next and link_next.get("href") and is_valid_href(link_next["href"]):
        joined = urllib.parse.urljoin(current_url, link_next["href"])
        if same_domain(joined, current_url):
            return joined
    a_next = soup.find("a", rel=lambda v: v and "next" in str(v).lower(), href=True)
    if a_next and is_valid_href(a_next["href"]):
        joined = urllib.parse.urljoin(current_url, a_next["href"])
        if same_domain(joined, current_url):
            return joined
    tokens = rule.listing_next_hint_tokens if rule else ("Ê¨°„Å∏", "Ê¨°„ÅÆ", "„ÇÇ„Å£„Å®Ë¶ã„Çã", "Next", "More")
    for a in soup.find_all("a", href=True):
        try:
            txt = a.get_text(strip=True)
        except Exception:
            continue
        if any(t in txt for t in tokens):
            href = a.get("href")
            if href and is_valid_href(href):
                joined = urllib.parse.urljoin(current_url, href)
                if same_domain(joined, current_url):
                    return joined
    return None

def is_article_url(url: str, rule: Optional[SiteRule]) -> bool:
    if not rule:
        return True
    pu = urllib.parse.urlparse(url)
    path = pu.path or ""
    low = path.lower()
    if any(low.startswith(p) for p in rule.deny_path_prefixes):
        return False
    return bool(rule.article_path_allow.search(path))

def extract_article_links_from_listing(
    soup: BeautifulSoup,
    current_url: str,
    rule: Optional[SiteRule],
    link_limit: int = 80
) -> List[str]:
    base = urllib.parse.urlparse(current_url)
    out: List[str] = []
    seen: Set[str] = set()
    for a in soup.find_all("a", href=True):
        href = a.get("href")
        if not is_valid_href(href):
            continue
        url = urllib.parse.urljoin(current_url, href)
        pu = urllib.parse.urlparse(url)
        if pu.netloc != base.netloc:
            continue
        if not is_article_url(url, rule):
            continue
        if url not in seen:
            seen.add(url)
            out.append(url)
        if len(out) >= link_limit:
            break
    return out

# ------------------------------------------------------------
# Metadata Extraction
# ------------------------------------------------------------
def extract_release_date(soup: BeautifulSoup) -> str:
    meta_selectors = [
        ("meta", {"property": "article:published_time"}),
        ("meta", {"property": "og:published_time"}),
        ("meta", {"name": "pubdate"}),
        ("meta", {"name": "publishdate"}),
        ("meta", {"name": "date"}),
        ("meta", {"itemprop": "datePublished"}),
    ]
    for tag_name, attrs in meta_selectors:
        m = soup.find(tag_name, attrs=attrs)
        if m and m.get("content"):
            return normalize_date(str(m["content"]))
    t = soup.find("time")
    if t:
        dt = t.get("datetime")
        if dt:
            return normalize_date(str(dt))
        txt = t.get_text(strip=True)
        if txt:
            return normalize_date(txt)
    return ""

def _as_list(x: Any) -> List[Any]:
    if x is None: return []
    return x if isinstance(x, list) else [x]

def extract_location_from_jsonld(soup: BeautifulSoup) -> Dict[str, str]:
    out = {"address": "", "latitude": "", "longitude": ""}
    for sc in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            raw = sc.string or sc.get_text(strip=True)
            if not raw: continue
            obj = json.loads(raw)
        except: continue
        nodes: List[Any] = []
        for node in _as_list(obj):
            if isinstance(node, dict) and "@graph" in node:
                nodes.extend(_as_list(node.get("@graph")))
            else:
                nodes.append(node)
        for n in nodes:
            if not isinstance(n, dict): continue
            loc = n.get("location") or n.get("Place") or n.get("place")
            for loc_node in _as_list(loc):
                if not isinstance(loc_node, dict): continue
                addr = loc_node.get("address")
                if isinstance(addr, dict):
                    parts = [addr.get("addressRegion"), addr.get("addressLocality"), addr.get("streetAddress")]
                    addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                    if addr_text and not out["address"]: out["address"] = addr_text
                elif isinstance(addr, str) and addr.strip() and not out["address"]:
                    out["address"] = addr.strip()
                geo = loc_node.get("geo")
                if isinstance(geo, dict):
                    lat, lon = geo.get("latitude"), geo.get("longitude")
                    if lat is not None and not out["latitude"]: out["latitude"] = str(lat).strip()
                    if lon is not None and not out["longitude"]: out["longitude"] = str(lon).strip()
            if out["address"] or out["latitude"]: return out
    return out

# ------------------------------------------------------------
# Gemini Extraction (Single Article)
# ------------------------------------------------------------
def ai_extract_events_from_text(
    client: genai.Client,
    model_name: str,
    temperature: float,
    text: str,
    today: datetime.date,
    debug_mode: bool,
    gemini_error_counter: Dict[str, int],
    min_chunk_len: int = 120,
) -> List[Dict]:
    all_items: List[Dict] = []
    
    for chunk in split_text_into_chunks(text, chunk_size=8000, overlap=400):
        if not chunk or len(chunk) < min_chunk_len:
            continue

        prompt = f"""
‰ª•‰∏ã„ÅÆWeb„Éö„Éº„Ç∏Êú¨Êñá„Åã„Çâ„ÄÅ„Ç§„Éô„É≥„Éà„Éª„Éã„É•„Éº„ÇπÊÉÖÂ†±„ÇíJSONÈÖçÂàó„ÅßÊºè„Çå„Å™„ÅèÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
„ÄêÁèæÂú®Êó•‰ªò: {today}„Äë

[ÊäΩÂá∫„É´„Éº„É´]
- Êú¨Êñá„Å´Âê´„Åæ„Çå„Çã„Ç§„Éô„É≥„ÉàÔºàÂ±ïÁ§∫„ÄÅÂÇ¨‰∫ã„ÄÅ„Ç≠„É£„É≥„Éö„Éº„É≥„ÄÅÂãüÈõÜ„ÄÅÁô∫Ë°®‰ºö„ÄÅ„Çª„Éü„Éä„ÉºÁ≠âÔºâ„ÇÑ„ÄÅÊó•ÊôÇ„ÉªÊúüÈñì„ÉªÂ†¥ÊâÄ„ÅåÊõ∏„Åã„Çå„Å¶„ÅÑ„ÇãÊÉÖÂ†±„ÇíÂèØËÉΩ„Å™Èôê„ÇäÊäΩÂá∫„ÄÇ
- ÁúÅÁï•Âé≥Á¶Å„ÄÇ„Åü„Å†„Åó„Äå‰ºÅÊ•≠„Éï„ÉÉ„Çø„ÉªÂïè„ÅÑÂêà„Çè„ÅõÂÖà„ÉÜ„É≥„Éó„É¨„Äç„Å™„Å©„ÅÆÈùû„Ç§„Éô„É≥„ÉàÂÆöÂûãÊñá„ÅØÁÑ°ÁêÜ„Å´Êãæ„Çè„Å™„ÅÑ„ÄÇ
- date_info „ÅØÊú¨Êñá„ÅÆË°®Ë®ò„ÅÆ„Åæ„Åæ„Åß„ÇÇËâØ„ÅÑ„Åå„ÄÅÂèØËÉΩ„Å™„Çâ YYYYÂπ¥MMÊúàDDÊó• / YYYY/MM/DD / ÊúüÈñìË°®ÁèæÔºà‰æã: 2025Âπ¥01Êúà01Êó•„Äú2025Âπ¥02Êúà01Êó•Ôºâ„ÄÇ
- address / latitude / longitude „ÅØÊú¨Êñá„Åã„ÇâÊé®ÂÆö„Åß„Åç„ÇãÁØÑÂõ≤„Åß„Çà„ÅÑÔºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ„ÄÇ
- Âá∫Âäõ„ÅØÂøÖ„ÅöJSON„ÅÆ„ÅøÔºàË™¨ÊòéÊñá„ÅØÁ¶ÅÊ≠¢Ôºâ„ÄÇ

[JSONÂΩ¢Âºè]
[
  {{
    "name": "„Çø„Ç§„Éà„É´",
    "place": "Â†¥ÊâÄÔºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ",
    "address": "‰ΩèÊâÄÔºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ",
    "latitude": "Á∑ØÂ∫¶Ôºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ",
    "longitude": "ÁµåÂ∫¶Ôºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ",
    "date_info": "Êó•‰ªò„ÇÑÊúüÈñìÔºà‰∏çÊòé„Å™„ÇâÁ©∫ÊñáÂ≠óÔºâ",
    "description": "Ê¶ÇË¶ÅÔºàÁü≠„ÇÅ„Å´Ôºâ"
  }}
]

Êú¨Êñá:
{chunk}
"""
        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                res = client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        temperature=float(temperature)
                    )
                )

                if debug_mode:
                    st.write("üß™ Gemini raw (head 400):", (res.text or "")[:400])

                extracted = safe_json_parse(res.text)
                if isinstance(extracted, list):
                    for item in extracted:
                        if not item or not isinstance(item, dict):
                            continue
                        name = str(item.get("name") or "").strip()
                        if not name:
                            continue
                        out = {
                            "name": name,
                            "place": str(item.get("place") or "").strip(),
                            "address": str(item.get("address") or "").strip(),
                            "latitude": str(item.get("latitude") or "").strip(),
                            "longitude": str(item.get("longitude") or "").strip(),
                            "date_info": normalize_date(str(item.get("date_info") or "").strip()),
                            "description": str(item.get("description") or "").strip(),
                        }
                        all_items.append(out)
                break 

            except Exception as e:
                err_str = str(e)
                if "429" in err_str or "RESOURCE_EXHAUSTED" in err_str:
                    if attempt < max_retries:
                        wait_time = 10 * (attempt + 1)
                        if debug_mode:
                            st.warning(f"‚ö†Ô∏è 429 Detected. Retrying in {wait_time}s... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                        if debug_mode: st.error(f"‚ùå Retry Limit Reached: {e}")
                else:
                    gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                    if debug_mode: st.error(f"‚ùå Gemini Error: {e}")
                    else: st.warning(f"‚ùå Gemini Error: {e}")
                    break

    return all_items

# ============================================================
# Sidebar UI
# ============================================================
with st.sidebar:
    st.header("1. ÂØæË±°„Çµ„Ç§„Éà")
    PRESET_URLS = {
        "PRTIMES („Ç∞„É´„É°)": "https://prtimes.jp/gourmet/",
        "PRTIMES („Ç®„É≥„Çø„É°)": "https://prtimes.jp/entertainment/",
        "AtPress („Ç∞„É´„É°)": "https://www.atpress.ne.jp/news/food",
        "AtPress (Êñ∞ÁùÄ)": "https://www.atpress.ne.jp/news",
    }
    selected_presets = st.multiselect("„Éó„É™„Çª„ÉÉ„Éà", list(PRESET_URLS.keys()), default=["PRTIMES („Ç∞„É´„É°)"])
    st.markdown("### üîó „Ç´„Çπ„Çø„É†URL")
    custom_urls_text = st.text_area("URLÔºà1Ë°å„Å´1„Å§Ôºâ", height=110)

    st.divider()
    st.header("2. Êé¢Á¥¢Ë®≠ÂÆö")
    max_pages = st.slider("‰∏ÄË¶ß„ÅÆÊúÄÂ§ß„Éö„Éº„Ç∏Êï∞", 1, 30, 6)
    link_limit_per_page = st.slider("1„Éö„Éº„Ç∏„ÅÇ„Åü„ÇäÂèéÈõÜURL‰∏äÈôê", 10, 300, 80)
    max_articles_total = st.slider("Á∑èË®ò‰∫ãÊï∞„ÅÆ‰∏äÈôê", 20, 2000, 400, step=20)
    sleep_sec = st.slider("„Ç¢„ÇØ„Çª„ÇπÈñìÈöîÔºàÁßíÔºâ", 0.0, 30.0, 10.0, step=1.0)
    
    st.divider()
    st.header("3. GeminiË®≠ÂÆö")
    model_name = st.text_input("„É¢„Éá„É´Âêç", value="gemini-2.0-flash-lite")
    temperature = st.slider("temperature", 0.0, 1.0, 0.0)

    st.divider()
    st.header("üêû „Éá„Éê„ÉÉ„Ç∞")
    debug_mode = st.checkbox("„Éá„Éê„ÉÉ„Ç∞„É¢„Éº„Éâ", value=False)
    debug_show_articles = st.slider("„Éá„Éê„ÉÉ„Ç∞Ë°®Á§∫„Åô„ÇãË®ò‰∫ãÊï∞", 1, 10, 3)

    st.divider()
    st.header("4. ÈáçË§áÈô§Â§ñ")
    uploaded_file = st.file_uploader("ÈÅéÂéªCSV", type="csv")
    
    st.divider()
    if st.button("„É¢„Éá„É´ÂêçË®∫Êñ≠"):
        api_key_check = os.environ.get("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
        if not api_key_check: st.error("No API Key")
        else:
            try:
                cl = genai.Client(api_key=api_key_check)
                ms = [getattr(m, "name", str(m)).replace("models/", "") for m in cl.models.list() if "gemini" in str(m).lower()]
                st.code("\n".join(sorted(ms)))
            except Exception as e: st.error(e)

# ============================================================
# Main Logic
# ============================================================
existing_fingerprints = set()
if uploaded_file:
    try:
        edf = pd.read_csv(uploaded_file)
        for _, r in edf.iterrows():
            n = normalize_string(r.get("„Ç§„Éô„É≥„ÉàÂêç", "") or r.get("name", ""))
            if n: existing_fingerprints.add(n)
        st.sidebar.success(f"üìö {len(existing_fingerprints)}‰ª∂„É≠„Éº„ÉâÊ∏à")
    except: pass

if "extracted_data" not in st.session_state: st.session_state.extracted_data = None

if st.button("‰∏ÄÊã¨Ë™≠„ÅøËæº„ÅøÈñãÂßã", type="primary"):
    today = datetime.date.today()
    api_key = os.environ.get("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        st.error("API KeyÊú™Ë®≠ÂÆö")
        st.stop()
        
    targets = [{"url": PRESET_URLS[k], "label": k} for k in selected_presets]
    if custom_urls_text:
        for u in custom_urls_text.splitlines():
            if u.strip().startswith("http"): targets.append({"url": u.strip(), "label": "Custom"})
    
    if not targets: st.error("URL„Å™„Åó"); st.stop()
    
    client = genai.Client(api_key=api_key)
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    
    status = st.empty()
    progress = st.progress(0.0)
    
    # 1. Collect URLs
    collected = []
    visited = set()
    seen_urls = set()
    
    for t in targets:
        curr = t["url"]
        rule = get_site_rule(curr)
        for p in range(max_pages):
            if curr in visited: break
            visited.add(curr)
            status.info(f"üìÑ ‰∏ÄË¶ßÂèñÂæó: {t['label']} ({p+1}/{max_pages})")
            
            html = fetch_html(session, curr)
            if not html: break
            soup = BeautifulSoup(html, "html.parser")
            
            links = extract_article_links_from_listing(soup, curr, rule, link_limit_per_page)
            for u in links:
                if u not in seen_urls:
                    seen_urls.add(u)
                    collected.append((u, t["label"]))
            
            if len(collected) >= max_articles_total: break
            curr = find_next_page_url(soup, curr, rule)
            if not curr: break
            time.sleep(1.0)
        if len(collected) >= max_articles_total: break
            
    collected = collected[:max_articles_total]
    if not collected: st.error("Ë®ò‰∫ãURL„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü"); st.stop()
    
    # 2. Extract Events
    extracted_all = []
    run_fingerprints = set()
    gemini_error_counter = {"count": 0}
    
    status.info(f"üß† Ë®ò‰∫ãËß£ÊûêÈñãÂßã: {len(collected)}‰ª∂")
    
    for i, (url, label) in enumerate(collected):
        progress.progress((i+1) / len(collected))
        status.info(f"üß† Ëß£Êûê‰∏≠ ({i+1}/{len(collected)}): {url}")
        
        rule = get_site_rule(url)
        if not is_article_url(url, rule): continue
        html = fetch_html(session, url)
        if not html: continue
        
        soup = BeautifulSoup(html, "html.parser")
        r_date = extract_release_date(soup)
        loc = extract_location_from_jsonld(soup)
        clean_soup(soup)
        text = extract_main_text(soup, rule)
        
        if not text or len(text) < 200: continue
        
        if debug_mode and i < debug_show_articles:
            st.write(f"üß™ [debug] len={len(text)} date={r_date} loc={loc}")

        items = ai_extract_events_from_text(
            client, model_name, temperature, 
            text, today, debug_mode and (i < debug_show_articles), gemini_error_counter
        )
            
        for item in items:
            item["release_date"] = r_date
            if loc.get("address") and not item.get("address"): item["address"] = loc["address"]
            if loc.get("latitude") and not item.get("latitude"): item["latitude"] = loc["latitude"]
            if loc.get("longitude") and not item.get("longitude"): item["longitude"] = loc["longitude"]

            fp = normalize_string(item["name"])
            if fp in existing_fingerprints or fp in run_fingerprints: continue
            run_fingerprints.add(fp)
            
            item["source_label"] = label
            item["source_url"] = url
            extracted_all.append(item)
        
        time.sleep(sleep_sec)
            
    st.session_state.extracted_data = extracted_all
    st.session_state.last_update = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    status.success(f"ÂÆå‰∫Ü! {len(extracted_all)}‰ª∂ÊäΩÂá∫")

if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)
    
    rubbish = ["Á©∫ÊñáÂ≠ó", "‰∏çÊòé", "None", "null", "N/A", "Êú™ÂÆö"]
    for c in df.columns:
        df[c] = df[c].replace(rubbish, "")

    display_df = df.rename(columns={
        "release_date": "„É™„É™„Éº„ÇπÊó•", "date_info": "ÊúüÈñì", "name": "„Ç§„Éô„É≥„ÉàÂêç",
        "place": "Â†¥ÊâÄ", "address": "‰ΩèÊâÄ", "latitude": "Á∑ØÂ∫¶", "longitude": "ÁµåÂ∫¶",
        "description": "Ê¶ÇË¶Å", "source_label": "ÊÉÖÂ†±Ê∫ê", "source_url": "URL"
    })
    
    cols = ["„É™„É™„Éº„ÇπÊó•", "ÊúüÈñì", "„Ç§„Éô„É≥„ÉàÂêç", "Â†¥ÊâÄ", "‰ΩèÊâÄ", "Á∑ØÂ∫¶", "ÁµåÂ∫¶", "Ê¶ÇË¶Å", "ÊÉÖÂ†±Ê∫ê", "URL"]
    display_df = display_df[[c for c in cols if c in display_df.columns]]
    
    st.dataframe(display_df, use_container_width=True, hide_index=True,
                 column_config={"URL": st.column_config.LinkColumn("Link")})
    st.download_button("CSV DL", display_df.to_csv(index=False).encode("utf-8_sig"), "events_stable.csv")
