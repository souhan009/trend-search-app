import streamlit as st
import datetime
import os
import json
import time
import re
import urllib.parse
import csv
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set, Any

import pandas as pd
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag

from google import genai
from google.genai import types

# ============================================================
# Streamlit config
# ============================================================
st.set_page_config(page_title="ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºï¼ˆè‡ªå‹•ä¿å­˜ç‰ˆï¼‰", page_icon="ğŸ’¾", layout="wide")
st.title("ğŸ’¾ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆè‡ªå‹•ä¿å­˜ç‰ˆï¼‰")
st.markdown("""
**AI Ã— ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆé€”ä¸­ä¿å­˜å¯¾å¿œï¼‰** 1ä»¶æŠ½å‡ºã™ã‚‹ã”ã¨ã«ã€è‡ªå‹•çš„ã« `progressive_results.csv` ã«ä¿å­˜ã—ã¾ã™ã€‚  
é€”ä¸­ã§ã‚¨ãƒ©ãƒ¼åœæ­¢ã—ã¦ã‚‚ã€ãã“ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã¯ç¢ºä¿ã•ã‚Œã¾ã™ã€‚
""")

# ============================================================
# Site rules
# ============================================================
@dataclass(frozen=True)
class SiteRule:
    name: str
    match_netloc: str
    article_path_allow: re.Pattern
    listing_next_hint_tokens: Tuple[str, ...] = ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "NEXT", "More", "MORE")
    deny_path_prefixes: Tuple[str, ...] = ("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account")
    content_selectors: Tuple[str, ...] = ("article", "main", "div.article", "div#main", "div.content")

SITE_RULES: List[SiteRule] = [
    SiteRule(
        name="PRTIMES",
        match_netloc="prtimes.jp",
        article_path_allow=re.compile(r"^/main/html/rd/p/"),
        deny_path_prefixes=(
            "/ranking", "/company", "/categories", "/category", "/tag", "/tags",
            "/gourmet", "/entertainment", "/fashion", "/beauty", "/sports", "/technology", "/topics",
        ),
        content_selectors=("article", "main", "div.main-contents", "div#main", "div.body", "div.content")
    ),
    SiteRule(
        name="AtPress",
        match_netloc="atpress.ne.jp",
        article_path_allow=re.compile(r"^/news/\d+"),
        deny_path_prefixes=("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account"),
        content_selectors=("article", "main", "div#main", "div.newsDetail", "div.content")
    ),
]

def get_site_rule(url: str) -> Optional[SiteRule]:
    netloc = urllib.parse.urlparse(url).netloc.lower()
    for rule in SITE_RULES:
        if rule.match_netloc in netloc:
            return rule
    return None

# ============================================================
# Utils
# ============================================================
def normalize_date(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    t = text.strip()
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", t)
    if m:
        y, mo, d = m.group(1), m.group(2).zfill(2), m.group(3).zfill(2)
        if "/" in t:
            return f"{y}/{mo}/{d}"
        return f"{y}å¹´{mo}æœˆ{d}æ—¥"
    def rep_ymd(m2):
        return f"{m2.group(1)}å¹´{m2.group(2).zfill(2)}æœˆ{m2.group(3).zfill(2)}æ—¥"
    t = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", rep_ymd, t)
    t = re.sub(r"(\d{4})/(\d{1,2})/(\d{1,2})", lambda m2: f"{m2.group(1)}/{m2.group(2).zfill(2)}/{m2.group(3).zfill(2)}", t)
    return t.strip()

def normalize_string(text) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace(" ", "").replace("ã€€", "")
    t = t.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return t.lower().strip()

def safe_json_parse(json_str: str) -> List[Dict]:
    if not json_str or not isinstance(json_str, str):
        return []
    s = json_str.replace("```json", "").replace("```", "").strip()
    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, list) else []
        except Exception:
            pass
    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return [obj] if isinstance(obj, dict) else []
        except Exception:
            pass
    return []

def is_valid_href(href: str) -> bool:
    if not href:
        return False
    h = href.strip()
    if h.startswith("#"):
        return False
    if h.lower().startswith("javascript:"):
        return False
    return True

def same_domain(url_a: str, url_b: str) -> bool:
    try:
        return urllib.parse.urlparse(url_a).netloc == urllib.parse.urlparse(url_b).netloc
    except Exception:
        return False

def fetch_html(session: requests.Session, url: str, timeout=(5, 20), max_retries=2) -> Optional[str]:
    for attempt in range(max_retries + 1):
        try:
            r = session.get(url, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            if r.status_code in (429, 503) and attempt < max_retries:
                time.sleep(1.2 * (attempt + 1))
                continue
            return None
        except requests.RequestException:
            if attempt < max_retries:
                time.sleep(1.0 * (attempt + 1))
                continue
            return None
    return None

def clean_soup(soup: BeautifulSoup) -> None:
    for t in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
        try:
            t.decompose()
        except Exception:
            pass
    exclude_tokens = ["sidebar", "ranking", "recommend", "widget", "ad", "bread", "breadcrumb", "banner"]
    for t in soup.find_all(True):
        if not isinstance(t, Tag):
            continue
        attrs = getattr(t, "attrs", None)
        if not isinstance(attrs, dict):
            continue
        cls_list = attrs.get("class") or []
        if not isinstance(cls_list, (list, tuple)):
            cls_list = [str(cls_list)]
        cls = " ".join(map(str, cls_list)).lower()
        if any(tok in cls for tok in exclude_tokens):
            try:
                t.decompose()
            except Exception:
                pass

def extract_main_text(soup: BeautifulSoup, rule: Optional[SiteRule]) -> str:
    if rule:
        for sel in rule.content_selectors:
            try:
                node = soup.select_one(sel)
                if node:
                    return node.get_text("\n", strip=True)
            except Exception:
                continue
    return soup.get_text("\n", strip=True)

def split_text_into_chunks(text: str, chunk_size=8000, overlap=400):
    if not text:
        return
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        yield text[start:end]
        start = max(end - overlap, end)

def find_next_page_url(soup: BeautifulSoup, current_url: str, rule: Optional[SiteRule]) -> Optional[str]:
    link_next = soup.find("link", rel="next")
    if link_next and link_next.get("href") and is_valid_href(link_next["href"]):
        joined = urllib.parse.urljoin(current_url, link_next["href"])
        if same_domain(joined, current_url):
            return joined
    a_next = soup.find("a", rel=lambda v: v and "next" in str(v).lower(), href=True)
    if a_next and is_valid_href(a_next["href"]):
        joined = urllib.parse.urljoin(current_url, a_next["href"])
        if same_domain(joined, current_url):
            return joined
    tokens = rule.listing_next_hint_tokens if rule else ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "More")
    for a in soup.find_all("a", href=True):
        try:
            txt = a.get_text(strip=True)
        except Exception:
            continue
        if any(t in txt for t in tokens):
            href = a.get("href")
            if href and is_valid_href(href):
                joined = urllib.parse.urljoin(current_url, href)
                if same_domain(joined, current_url):
                    return joined
    return None

def is_article_url(url: str, rule: Optional[SiteRule]) -> bool:
    if not rule:
        return True
    pu = urllib.parse.urlparse(url)
    path = pu.path or ""
    low = path.lower()
    if any(low.startswith(p) for p in rule.deny_path_prefixes):
        return False
    return bool(rule.article_path_allow.search(path))

def extract_article_links_from_listing(
    soup: BeautifulSoup,
    current_url: str,
    rule: Optional[SiteRule],
    link_limit: int = 80
) -> List[str]:
    base = urllib.parse.urlparse(current_url)
    out: List[str] = []
    seen: Set[str] = set()
    for a in soup.find_all("a", href=True):
        href = a.get("href")
        if not is_valid_href(href):
            continue
        url = urllib.parse.urljoin(current_url, href)
        pu = urllib.parse.urlparse(url)
        if pu.netloc != base.netloc:
            continue
        if not is_article_url(url, rule):
            continue
        if url not in seen:
            seen.add(url)
            out.append(url)
        if len(out) >= link_limit:
            break
    return out

# ------------------------------------------------------------
# Metadata Extraction
# ------------------------------------------------------------
def extract_release_date(soup: BeautifulSoup) -> str:
    meta_selectors = [
        ("meta", {"property": "article:published_time"}),
        ("meta", {"property": "og:published_time"}),
        ("meta", {"name": "pubdate"}),
        ("meta", {"name": "publishdate"}),
        ("meta", {"name": "date"}),
        ("meta", {"itemprop": "datePublished"}),
    ]
    for tag_name, attrs in meta_selectors:
        m = soup.find(tag_name, attrs=attrs)
        if m and m.get("content"):
            return normalize_date(str(m["content"]))
    t = soup.find("time")
    if t:
        dt = t.get("datetime")
        if dt:
            return normalize_date(str(dt))
        txt = t.get_text(strip=True)
        if txt:
            return normalize_date(txt)
    return ""

def _as_list(x: Any) -> List[Any]:
    if x is None: return []
    return x if isinstance(x, list) else [x]

def extract_location_from_jsonld(soup: BeautifulSoup) -> Dict[str, str]:
    out = {"address": "", "latitude": "", "longitude": ""}
    for sc in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            raw = sc.string or sc.get_text(strip=True)
            if not raw: continue
            obj = json.loads(raw)
        except: continue
        nodes: List[Any] = []
        for node in _as_list(obj):
            if isinstance(node, dict) and "@graph" in node:
                nodes.extend(_as_list(node.get("@graph")))
            else:
                nodes.append(node)
        for n in nodes:
            if not isinstance(n, dict): continue
            loc = n.get("location") or n.get("Place") or n.get("place")
            for loc_node in _as_list(loc):
                if not isinstance(loc_node, dict): continue
                addr = loc_node.get("address")
                if isinstance(addr, dict):
                    parts = [addr.get("addressRegion"), addr.get("addressLocality"), addr.get("streetAddress")]
                    addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                    if addr_text and not out["address"]: out["address"] = addr_text
                elif isinstance(addr, str) and addr.strip() and not out["address"]:
                    out["address"] = addr.strip()
                geo = loc_node.get("geo")
                if isinstance(geo, dict):
                    lat, lon = geo.get("latitude"), geo.get("longitude")
                    if lat is not None and not out["latitude"]: out["latitude"] = str(lat).strip()
                    if lon is not None and not out["longitude"]: out["longitude"] = str(lon).strip()
            if out["address"] or out["latitude"]: return out
    return out

# ------------------------------------------------------------
# Gemini Extraction (Single Article)
# ------------------------------------------------------------
def ai_extract_events_from_text(
    client: genai.Client,
    model_name: str,
    temperature: float,
    text: str,
    today: datetime.date,
    debug_mode: bool,
    gemini_error_counter: Dict[str, int],
    min_chunk_len: int = 120,
) -> List[Dict]:
    all_items: List[Dict] = []
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒUIã§æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«åãŒãã®ã¾ã¾ä½¿ã‚ã‚Œã¾ã™
    for chunk in split_text_into_chunks(text, chunk_size=8000, overlap=400):
        if not chunk or len(chunk) < min_chunk_len:
            continue

        prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’JSONé…åˆ—ã§æ¼ã‚ŒãªãæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã€ç¾åœ¨æ—¥ä»˜: {today}ã€‘

[æŠ½å‡ºãƒ«ãƒ¼ãƒ«]
- æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå±•ç¤ºã€å‚¬äº‹ã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€å‹Ÿé›†ã€ç™ºè¡¨ä¼šã€ã‚»ãƒŸãƒŠãƒ¼ç­‰ï¼‰ã‚„ã€æ—¥æ™‚ãƒ»æœŸé–“ãƒ»å ´æ‰€ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å¯èƒ½ãªé™ã‚ŠæŠ½å‡ºã€‚
- çœç•¥å³ç¦ã€‚ãŸã ã—ã€Œä¼æ¥­ãƒ•ãƒƒã‚¿ãƒ»å•ã„åˆã‚ã›å…ˆãƒ†ãƒ³ãƒ—ãƒ¬ã€ãªã©ã®éã‚¤ãƒ™ãƒ³ãƒˆå®šå‹æ–‡ã¯ç„¡ç†ã«æ‹¾ã‚ãªã„ã€‚
- date_info ã¯æœ¬æ–‡ã®è¡¨è¨˜ã®ã¾ã¾ã§ã‚‚è‰¯ã„ãŒã€å¯èƒ½ãªã‚‰ YYYYå¹´MMæœˆDDæ—¥ / YYYY/MM/DD / æœŸé–“è¡¨ç¾ï¼ˆä¾‹: 2025å¹´01æœˆ01æ—¥ã€œ2025å¹´02æœˆ01æ—¥ï¼‰ã€‚
- address / latitude / longitude ã¯æœ¬æ–‡ã‹ã‚‰æ¨å®šã§ãã‚‹ç¯„å›²ã§ã‚ˆã„ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰ã€‚
- å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ï¼ˆèª¬æ˜æ–‡ã¯ç¦æ­¢ï¼‰ã€‚

[JSONå½¢å¼]
[
  {{
    "name": "ã‚¿ã‚¤ãƒˆãƒ«",
    "place": "å ´æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "address": "ä½æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "latitude": "ç·¯åº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "longitude": "çµŒåº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "date_info": "æ—¥ä»˜ã‚„æœŸé–“ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "description": "æ¦‚è¦ï¼ˆçŸ­ã‚ã«ï¼‰"
  }}
]

æœ¬æ–‡:
{chunk}
"""
        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                res = client.models.generate_content(
                    model=model_name, # ã“ã“ã§UIã®å…¥åŠ›å€¤ãŒä½¿ã‚ã‚Œã¾ã™
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        temperature=float(temperature)
                    )
                )

                if debug_mode:
                    st.write("ğŸ§ª Gemini raw (head 400):", (res.text or "")[:400])

                extracted = safe_json_parse(res.text)
                if isinstance(extracted, list):
                    for item in extracted:
                        if not item or not isinstance(item, dict):
                            continue
                        name = str(item.get("name") or "").strip()
                        if not name:
                            continue
                        out = {
                            "name": name,
                            "place": str(item.get("place") or "").strip(),
                            "address": str(item.get("address") or "").strip(),
                            "latitude": str(item.get("latitude") or "").strip(),
                            "longitude": str(item.get("longitude") or "").strip(),
                            "date_info": normalize_date(str(item.get("date_info") or "").strip()),
                            "description": str(item.get("description") or "").strip(),
                        }
                        all_items.append(out)
                break 

            except Exception as e:
                err_str = str(e)
                # 429ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                if "429" in err_str or "RESOURCE_EXHAUSTED" in err_str:
                    if attempt < max_retries:
                        wait_time = 15 * (attempt + 1)
                        if debug_mode:
                            st.warning(f"âš ï¸ 429 Detected. Retrying in {wait_time}s... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                        if debug_mode: st.error(f"âŒ Retry Limit Reached: {e}")
                else:
                    gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                    if debug_mode: st.error(f"âŒ Gemini Error: {e}")
                    else: st.warning(f"âŒ Gemini Error: {e}")
                    break

    return all_items

# ------------------------------------------------------------
# Append to CSV (Incremental Save)
# ------------------------------------------------------------
def append_to_csv(data: Dict, filename: str):
    fieldnames = [
        "release_date", "date_info", "name", "place", "address", 
        "latitude", "longitude", "description", "source_label", "source_url"
    ]
    file_exists = os.path.isfile(filename)
    try:
        with open(filename, mode='a', encoding='utf-8_sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            if not file_exists:
                writer.writeheader()
            writer.writerow(data)
    except Exception as e:
        print(f"CSV Write Error: {e}")

# ============================================================
# Sidebar UI
# ============================================================
with st.sidebar:
    st.header("1. å¯¾è±¡ã‚µã‚¤ãƒˆ")
    PRESET_URLS = {
        "PRTIMES (ã‚°ãƒ«ãƒ¡)": "https://prtimes.jp/gourmet/",
        "PRTIMES (ã‚¨ãƒ³ã‚¿ãƒ¡)": "https://prtimes.jp/entertainment/",
        "AtPress (ã‚°ãƒ«ãƒ¡)": "https://www.atpress.ne.jp/news/food",
        "AtPress (æ–°ç€)": "https://www.atpress.ne.jp/news",
    }
    selected_presets = st.multiselect("ãƒ—ãƒªã‚»ãƒƒãƒˆ", list(PRESET_URLS.keys()), default=["PRTIMES (ã‚°ãƒ«ãƒ¡)"])
    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area("URLï¼ˆ1è¡Œã«1ã¤ï¼‰", height=110)

    st.divider()
    st.header("2. æ¢ç´¢è¨­å®š")
    max_pages = st.slider("ä¸€è¦§ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°", 1, 30, 6)
    link_limit_per_page = st.slider("1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šåé›†URLä¸Šé™", 10, 300, 80)
    max_articles_total = st.slider("ç·è¨˜äº‹æ•°ã®ä¸Šé™", 20, 2000, 400, step=20)
    sleep_sec = st.slider("ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ï¼ˆç§’ï¼‰", 0.0, 30.0, 5.0, step=1.0)
    
    st.divider()
    st.header("3. Geminiè¨­å®š")
    # UIå…¥åŠ›æ¬„ (åˆæœŸå€¤ã¯ gemini-2.0-flash ã§ã™ãŒã€ç”»é¢ä¸Šã§å¤‰æ›´ã™ã‚Œã°ãã‚ŒãŒä½¿ã‚ã‚Œã¾ã™)
    model_name = st.text_input("ãƒ¢ãƒ‡ãƒ«å", value="gemini-2.0-flash") 
    temperature = st.slider("temperature", 0.0, 1.0, 0.0)

    st.divider()
    st.header("ğŸ ãƒ‡ãƒãƒƒã‚°")
    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=False)
    debug_show_articles = st.slider("ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã™ã‚‹è¨˜äº‹æ•°", 1, 10, 3)

    st.divider()
    st.header("4. é‡è¤‡é™¤å¤–")
    uploaded_file = st.file_uploader("éå»CSV", type="csv")
    
    st.divider()
    if st.button("ãƒ¢ãƒ‡ãƒ«åè¨ºæ–­"):
        api_key_check = os.environ.get("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
        if not api_key_check: st.error("No API Key")
        else:
            try:
                cl = genai.Client(api_key=api_key_check)
                ms = [getattr(m, "name", str(m)).replace("models/", "") for m in cl.models.list() if "gemini" in str(m).lower()]
                st.code("\n".join(sorted(ms)))
            except Exception as e: st.error(e)

# ============================================================
# Main Logic
# ============================================================
existing_fingerprints = set()
if uploaded_file:
    try:
        edf = pd.read_csv(uploaded_file)
        for _, r in edf.iterrows():
            n = normalize_string(r.get("ã‚¤ãƒ™ãƒ³ãƒˆå", "") or r.get("name", ""))
            if n: existing_fingerprints.add(n)
        st.sidebar.success(f"ğŸ“š {len(existing_fingerprints)}ä»¶ãƒ­ãƒ¼ãƒ‰æ¸ˆ")
    except: pass

if "extracted_data" not in st.session_state: st.session_state.extracted_data = None

PROGRESSIVE_CSV = "progressive_results.csv"

if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    # æ—¢å­˜ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆãƒªã‚»ãƒƒãƒˆï¼‰
    if os.path.exists(PROGRESSIVE_CSV):
        try:
            os.remove(PROGRESSIVE_CSV)
        except: pass

    today = datetime.date.today()
    api_key = os.environ.get("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        st.error("API Keyæœªè¨­å®š")
        st.stop()
        
    targets = [{"url": PRESET_URLS[k], "label": k} for k in selected_presets]
    if custom_urls_text:
        for u in custom_urls_text.splitlines():
            if u.strip().startswith("http"): targets.append({"url": u.strip(), "label": "Custom"})
    
    if not targets: st.error("URLãªã—"); st.stop()
    
    client = genai.Client(api_key=api_key)
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    
    status = st.empty()
    progress = st.progress(0.0)
    
    # 1. Collect URLs
    collected = []
    visited = set()
    seen_urls = set()
    
    for t in targets:
        curr = t["url"]
        rule = get_site_rule(curr)
        for p in range(max_pages):
            if curr in visited: break
            visited.add(curr)
            status.info(f"ğŸ“„ ä¸€è¦§å–å¾—: {t['label']} ({p+1}/{max_pages})")
            
            html = fetch_html(session, curr)
            if not html: break
            soup = BeautifulSoup(html, "html.parser")
            
            links = extract_article_links_from_listing(soup, curr, rule, link_limit_per_page)
            for u in links:
                if u not in seen_urls:
                    seen_urls.add(u)
                    collected.append((u, t["label"]))
            
            if len(collected) >= max_articles_total: break
            curr = find_next_page_url(soup, curr, rule)
            if not curr: break
            time.sleep(1.0)
        if len(collected) >= max_articles_total: break
            
    collected = collected[:max_articles_total]
    if not collected: st.error("è¨˜äº‹URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"); st.stop()
    
    # 2. Extract Events
    extracted_all = []
    run_fingerprints = set()
    gemini_error_counter = {"count": 0}
    
    status.info(f"ğŸ§  è¨˜äº‹è§£æé–‹å§‹: {len(collected)}ä»¶ -> çµæœã¯ {PROGRESSIVE_CSV} ã«è‡ªå‹•ä¿å­˜ã•ã‚Œã¾ã™")
    
    for i, (url, label) in enumerate(collected):
        progress.progress((i+1) / len(collected))
        status.info(f"ğŸ§  è§£æä¸­ ({i+1}/{len(collected)}) ãƒ¢ãƒ‡ãƒ«: {model_name} | URL: {url}")
        
        rule = get_site_rule(url)
        if not is_article_url(url, rule): continue
        html = fetch_html(session, url)
        if not html: continue
        
        soup = BeautifulSoup(html, "html.parser")
        r_date = extract_release_date(soup)
        loc = extract_location_from_jsonld(soup)
        clean_soup(soup)
        text = extract_main_text(soup, rule)
        
        if not text or len(text) < 200: continue
        
        if debug_mode and i < debug_show_articles:
            st.write(f"ğŸ§ª [debug] len={len(text)} date={r_date} loc={loc}")

        items = ai_extract_events_from_text(
            client, model_name, temperature, 
            text, today, debug_mode and (i < debug_show_articles), gemini_error_counter
        )
            
        for item in items:
            item["release_date"] = r_date
            if loc.get("address") and not item.get("address"): item["address"] = loc["address"]
            if loc.get("latitude") and not item.get("latitude"): item["latitude"] = loc["latitude"]
            if loc.get("longitude") and not item.get("longitude"): item["longitude"] = loc["longitude"]

            fp = normalize_string(item["name"])
            if fp in existing_fingerprints or fp in run_fingerprints: continue
            run_fingerprints.add(fp)
            
            item["source_label"] = label
            item["source_url"] = url
            
            extracted_all.append(item)
            append_to_csv(item, PROGRESSIVE_CSV)
        
        time.sleep(sleep_sec)
            
    st.session_state.extracted_data = extracted_all
    st.session_state.last_update = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    status.success(f"å®Œäº†! {len(extracted_all)}ä»¶æŠ½å‡ºã€‚ãƒ•ã‚¡ã‚¤ãƒ«: {PROGRESSIVE_CSV}")

if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)
    
    rubbish = ["ç©ºæ–‡å­—", "ä¸æ˜", "None", "null", "N/A", "æœªå®š"]
    for c in df.columns:
        df[c] = df[c].replace(rubbish, "")

    display_df = df.rename(columns={
        "release_date": "ãƒªãƒªãƒ¼ã‚¹æ—¥", "date_info": "æœŸé–“", "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
        "place": "å ´æ‰€", "address": "ä½æ‰€", "latitude": "ç·¯åº¦", "longitude": "çµŒåº¦",
        "description": "æ¦‚è¦", "source_label": "æƒ…å ±æº", "source_url": "URL"
    })
    
    cols = ["ãƒªãƒªãƒ¼ã‚¹æ—¥", "æœŸé–“", "ã‚¤ãƒ™ãƒ³ãƒˆå", "å ´æ‰€", "ä½æ‰€", "ç·¯åº¦", "çµŒåº¦", "æ¦‚è¦", "æƒ…å ±æº", "URL"]
    display_df = display_df[[c for c in cols if c in display_df.columns]]
    
    st.dataframe(display_df, use_container_width=True, hide_index=True,
                 column_config={"URL": st.column_config.LinkColumn("Link")})
    
    st.download_button("çµæœCSVã‚’DL", display_df.to_csv(index=False).encode("utf-8_sig"), "events_final.csv")
    
    if os.path.exists(PROGRESSIVE_CSV):
        with open(PROGRESSIVE_CSV, "rb") as f:
            st.download_button("é€”ä¸­çµŒéCSVã‚’DL", f, file_name="events_progressive.csv")
