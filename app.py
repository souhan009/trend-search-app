import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import re
import pydeck as pdk
import urllib.parse

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ—ºï¸")

st.title("ğŸ—ºï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆMapæ¤œç´¢")
st.markdown("æŒ‡å®šã—ãŸã€Œã‚¤ãƒ™ãƒ³ãƒˆã¾ã¨ã‚ã‚µã‚¤ãƒˆã€ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€æƒ…å ±ã‚’ä¸€æ‹¬æŠ½å‡ºã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    st.markdown("### ğŸ“ åœ°åŸŸãƒ»å ´æ‰€")
    region = st.text_input("æ¤œç´¢ã—ãŸã„å ´æ‰€", value="æ±äº¬éƒ½æ¸‹è°·åŒº", help="å…·ä½“çš„ãªåœ°åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.markdown("---")
    st.markdown("### ğŸ”— æ¤œç´¢å¯¾è±¡ãƒšãƒ¼ã‚¸æŒ‡å®š")
    
    # æ¤œç´¢å¯¾è±¡ãƒšãƒ¼ã‚¸
    SPECIFIC_PAGES = {
        "Let's Enjoy Tokyo (é–¢æ±ã‚¨ãƒªã‚¢ä¸€è¦§)": "https://www.enjoytokyo.jp/event/list/regn01/",
        "GO TOKYO (æ±äº¬å…¬å¼ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼)": "https://www.gotokyo.org/jp/event-calendar/",
        "Walkerplus (æ±äº¬ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§)": "https://www.walkerplus.com/event_list/ar0313/",
        "Fashion Press (ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§)": "https://www.fashion-press.net/news/",
        "TimeOut Tokyo (æ±äº¬ã®ã‚¤ãƒ™ãƒ³ãƒˆ)": "https://www.timeout.jp/tokyo/ja/things-to-do/",
        "Jorudan (ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±)": "https://www.jorudan.co.jp/sp/event/"
    }
    
    selected_pages = st.multiselect(
        "æ¤œç´¢ç¯„å›²ã¨ã™ã‚‹ãƒšãƒ¼ã‚¸ï¼ˆè¤‡æ•°å¯ï¼‰",
        options=list(SPECIFIC_PAGES.keys()),
        default=["Let's Enjoy Tokyo (é–¢æ±ã‚¨ãƒªã‚¢ä¸€è¦§)", "GO TOKYO (æ±äº¬å…¬å¼ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼)"]
    )
    
    st.info("ğŸ’¡ æŒ‡å®šã•ã‚ŒãŸURLéšå±¤ã®ä¸‹ã«ã‚ã‚‹æƒ…å ±ã®ã¿ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("æ¤œç´¢é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    if not selected_pages:
        st.error("âš ï¸ æ¤œç´¢å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’å°‘ãªãã¨ã‚‚1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # æ¤œç´¢å‡¦ç†
    client = genai.Client(api_key=api_key)
    status_text = st.empty()
    status_text.info(f"ğŸ” {region}ã®æƒ…å ±ã‚’ã€æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…ã‹ã‚‰å³å¯†ã«æ¤œç´¢ä¸­...")

    target_urls = [SPECIFIC_PAGES[name] for name in selected_pages]
    site_query = " OR ".join([f"site:{url}" for url in target_urls])
    
    today = datetime.date.today()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""
    ã‚ãªãŸã¯ã€ŒæŒ‡å®šã•ã‚ŒãŸWebãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’èª­ã¿å–ã‚‹ãƒ­ãƒœãƒƒãƒˆã€ã§ã™ã€‚
    ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½¿ã„ã€**æŒ‡å®šã•ã‚ŒãŸURLãƒ‘ã‚¹ã®é…ä¸‹ã«ã‚ã‚‹ãƒšãƒ¼ã‚¸**ã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
    ã€Œ{region} ã‚¤ãƒ™ãƒ³ãƒˆ é–‹å‚¬ä¸­ {site_query}ã€
    ã€Œ{region} æ–°è¦ã‚ªãƒ¼ãƒ—ãƒ³ {site_query}ã€

    ã€åŸºæº–æ—¥ã€‘
    æœ¬æ—¥ã¯ {today} ã§ã™ã€‚éå»ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

    ã€å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘
    1. **æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ãƒˆä»¥å¤–ã‹ã‚‰ã®æƒ…å ±ã¯çµ¶å¯¾ã«æ‹¾ã‚ãªã„ã§ãã ã•ã„ã€‚**
    2. **æé€ ç¦æ­¢**: æ¤œç´¢çµæœã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆåã¨æ—¥ä»˜ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    3. **URL**: è¨˜äº‹ã®å€‹åˆ¥URLãŒã‚ã‚Œã°ãã‚Œã‚’ã€ãªã‘ã‚Œã°ã€Œæ¤œç´¢çµæœã®URLï¼ˆä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLï¼‰ã€ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ¶ç©ºã®URLã¯ç¦æ­¢ã§ã™ã€‚

    ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
    [
        {{
            "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
            "place": "é–‹å‚¬å ´æ‰€",
            "date_info": "æœŸé–“(ä¾‹: é–‹å‚¬ä¸­ã€œ12/25)",
            "description": "æ¦‚è¦(çŸ­ãã¦OK)",
            "source_name": "ã‚µã‚¤ãƒˆå",
            "url": "URL",
            "lat": ç·¯åº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null),
            "lon": çµŒåº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null)
        }}
    ]
    """

    try:
        # AIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = client.models.generate_content(
            # â˜…ã“ã“ã‚’å¤‰æ›´: 2.0-flash -> 1.5-flash (åˆ¶é™ã«å¼•ã£ã‹ã‹ã‚Šã«ãã„ãƒ¢ãƒ‡ãƒ«)
            model="gemini-1.5-flash",
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
                response_mime_type="application/json",
                temperature=0.0
            )
        )

        status_text.empty()
        
        # --- JSONãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º ---
        text = response.text.replace("```json", "").replace("```", "").strip()
        data = []
        try:
            data = json.loads(text)
        except json.JSONDecodeError as e:
            try:
                if e.msg.startswith("Extra data"):
                    data = json.loads(text[:e.pos])
                else:
                    match = re.search(r'\[.*\]', text, re.DOTALL)
                    if match:
                        data = json.loads(match.group(0))
            except:
                pass
        
        # --- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & URLãƒã‚§ãƒƒã‚¯ ---
        cleaned_data = []
        for item in data:
            name = item.get('name', '')
            url = item.get('url', '')
            
            if not name or name.lower() in ['unknown', 'ã‚¤ãƒ™ãƒ³ãƒˆ']:
                continue
            
            # URLãƒã‚§ãƒƒã‚¯
            is_valid_source = False
            if url:
                for target in target_urls:
                    domain = urllib.parse.urlparse(target).netloc
                    if domain in url:
                        is_valid_source = True
                        break
            
            if not is_valid_source:
                search_query = f"{item['name']} {item['place']} ã‚¤ãƒ™ãƒ³ãƒˆ"
                item['url'] = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
                item['source_name'] = "Googleæ¤œç´¢"
            
            cleaned_data.append(item)
            
        data = cleaned_data

        if not data:
            st.warning(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸å†…ã‹ã‚‰ã¯ã€{region} ã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.info("åˆ¥ã®ã‚µã‚¤ãƒˆã‚’é¸æŠã™ã‚‹ã‹ã€ã‚¨ãƒªã‚¢åã‚’å¤‰æ›´ã—ã¦ï¼ˆä¾‹ï¼šæ¸‹è°·åŒºâ†’æ±äº¬ï¼‰è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
        df = pd.DataFrame(data)

        # --- 1. é«˜æ©Ÿèƒ½åœ°å›³ (Voyager) ---
        st.subheader(f"ğŸ“ {region}å‘¨è¾ºã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ—")
        st.caption(f"æŠ½å‡ºä»¶æ•°: {len(data)}ä»¶")
        
        if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
            map_df = df.dropna(subset=['lat', 'lon'])
            
            if not map_df.empty:
                view_state = pdk.ViewState(
                    latitude=map_df['lat'].mean(),
                    longitude=map_df['lon'].mean(),
                    zoom=13,
                    pitch=0,
                )

                layer = pdk.Layer(
                    "ScatterplotLayer",
                    map_df,
                    get_position='[lon, lat]',
                    get_color='[255, 75, 75, 160]',
                    get_radius=200,
                    pickable=True,
                )

                st.pydeck_chart(pdk.Deck(
                    map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                    initial_view_state=view_state,
                    layers=[layer],
                    tooltip={
                        "html": "<b>{name}</b><br/>{place}<br/><i>{description}</i>",
                        "style": {"backgroundColor": "steelblue", "color": "white"}
                    }
                ))
                st.caption("â€»åœ°å›³ä¸Šã®èµ¤ã„ä¸¸ã«ãƒã‚¦ã‚¹ã‚’ä¹—ã›ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
                
                # CSVä½œæˆ
                export_data = []
                for _, row in map_df.iterrows():
                    gaiyou = f"ã€æœŸé–“ã€‘{row.get('date_info')}\n{row.get('description')}"
                    export_data.append({
                        "Name": row.get('name'),
                        "ä½æ‰€": row.get('place'),
                        "æ¦‚è¦": gaiyou,
                        "å…¬å¼ã‚µã‚¤ãƒˆ": row.get('url', '')
                    })
                
                export_df = pd.DataFrame(export_data)
                csv = export_df.to_csv(index=False).encode('utf-8_sig')

                st.download_button(
                    label="ğŸ“¥ Googleãƒã‚¤ãƒãƒƒãƒ—ç”¨CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"event_map_{region}.csv",
                    mime='text/csv',
                    help="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Googleãƒã‚¤ãƒãƒƒãƒ—ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ã€Œä½æ‰€ã€åˆ—ã‚’ç›®å°ã®å ´æ‰€ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
                )
            else:
                 st.info("â€»ä½ç½®æƒ…å ±ãŒç‰¹å®šã§ããªã‹ã£ãŸãŸã‚ã€åœ°å›³ã«ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ãŒã€ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã«ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚")
        else:
            st.warning("åœ°å›³ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        # --- 2. é€Ÿå ±ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ ---
        st.markdown("---")
        st.subheader("ğŸ“‹ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ä¸€è¦§")
        
        for item in data:
            url_text = "ãªã—"
            source_label = item.get('source_name', 'æ²è¼‰ã‚µã‚¤ãƒˆ')
            
            link_label = f"{source_label} ã§è¦‹ã‚‹"
            if source_label == "Googleæ¤œç´¢":
                link_label = "ğŸ” Googleã§å†æ¤œç´¢"

            if item.get('url'):
                url_text = f"[ğŸ”— {link_label}]({item.get('url')})"

            st.markdown(f"""
            - **æœŸé–“**: {item.get('date_info')}
            - **ã‚¤ãƒ™ãƒ³ãƒˆå**: {item.get('name')}
            - **å ´æ‰€**: {item.get('place')}
            - **æ¦‚è¦**: {item.get('description')}
            - **ã‚½ãƒ¼ã‚¹**: {url_text}
            """)

    except Exception as e:
        status_text.empty()
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
