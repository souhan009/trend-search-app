import streamlit as st
import datetime
import os
import json
import time
import re
import urllib.parse
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set, Any

import pandas as pd
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag

from google import genai
from google.genai import types

# ============================================================
# Streamlit config
# ============================================================
st.set_page_config(page_title="ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆæ±ºå®šç‰ˆï¼‰", page_icon="ğŸ“–", layout="wide")
st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆæ±ºå®šç‰ˆï¼‰")
st.markdown("""
**AI Ã— ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæ±ºå®šç‰ˆï¼‰** ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã‚’å³å¯†ã«æŠ½å‡º â†’ æœ¬æ–‡ã‚’AIè§£æ â†’ é‡è¤‡é™¤å¤–ã—ã¦ä¸€è¦§åŒ–ã€‚  
**æ–°æ©Ÿèƒ½:** 1. **è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤**: APIåˆ¶é™(429)ãŒã‹ã‹ã£ã¦ã‚‚è‡ªå‹•ã§å¾…æ©Ÿã—ã¦å†é–‹ã—ã¾ã™ã€‚  
2. **ãƒ¢ãƒ‡ãƒ«è¨ºæ–­**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ä¸‹éƒ¨ã§ã€ã‚ãªãŸã®ç’°å¢ƒã§ä½¿ãˆã‚‹æ­£ç¢ºãªãƒ¢ãƒ‡ãƒ«åã‚’ç¢ºèªã§ãã¾ã™ã€‚
""")

# ============================================================
# Site rules
# ============================================================
@dataclass(frozen=True)
class SiteRule:
    name: str
    match_netloc: str
    article_path_allow: re.Pattern
    listing_next_hint_tokens: Tuple[str, ...] = ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "NEXT", "More", "MORE")
    deny_path_prefixes: Tuple[str, ...] = ("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account")
    content_selectors: Tuple[str, ...] = ("article", "main", "div.article", "div#main", "div.content")

SITE_RULES: List[SiteRule] = [
    SiteRule(
        name="PRTIMES",
        match_netloc="prtimes.jp",
        article_path_allow=re.compile(r"^/main/html/rd/p/"),
        deny_path_prefixes=(
            "/ranking", "/company", "/categories", "/category", "/tag", "/tags",
            "/gourmet", "/entertainment", "/fashion", "/beauty", "/sports", "/technology", "/topics",
        ),
        content_selectors=("article", "main", "div.main-contents", "div#main", "div.body", "div.content")
    ),
    SiteRule(
        name="AtPress",
        match_netloc="atpress.ne.jp",
        article_path_allow=re.compile(r"^/news/\d+"),
        deny_path_prefixes=("/ranking", "/tag", "/tags", "/category", "/categories", "/login", "/signup", "/account"),
        content_selectors=("article", "main", "div#main", "div.newsDetail", "div.content")
    ),
]

def get_site_rule(url: str) -> Optional[SiteRule]:
    netloc = urllib.parse.urlparse(url).netloc.lower()
    for rule in SITE_RULES:
        if rule.match_netloc in netloc:
            return rule
    return None

# ============================================================
# Utils
# ============================================================
def normalize_date(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    t = text.strip()
    # ISO format check
    m = re.search(r"(\d{4})[-/](\d{1,2})[-/](\d{1,2})", t)
    if m:
        y, mo, d = m.group(1), m.group(2).zfill(2), m.group(3).zfill(2)
        if "/" in t:
            return f"{y}/{mo}/{d}"
        return f"{y}å¹´{mo}æœˆ{d}æ—¥"

    def rep_ymd(m2):
        return f"{m2.group(1)}å¹´{m2.group(2).zfill(2)}æœˆ{m2.group(3).zfill(2)}æ—¥"

    t = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", rep_ymd, t)
    t = re.sub(r"(\d{4})/(\d{1,2})/(\d{1,2})", lambda m2: f"{m2.group(1)}/{m2.group(2).zfill(2)}/{m2.group(3).zfill(2)}", t)
    return t.strip()

def normalize_string(text) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace(" ", "").replace("ã€€", "")
    t = t.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return t.lower().strip()

def safe_json_parse(json_str: str) -> List[Dict]:
    if not json_str or not isinstance(json_str, str):
        return []
    s = json_str.replace("```json", "").replace("```", "").strip()

    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, list) else []
        except Exception:
            pass

    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return [obj] if isinstance(obj, dict) else []
        except Exception:
            pass

    return []

def is_valid_href(href: str) -> bool:
    if not href:
        return False
    h = href.strip()
    if h.startswith("#"):
        return False
    if h.lower().startswith("javascript:"):
        return False
    return True

def same_domain(url_a: str, url_b: str) -> bool:
    try:
        return urllib.parse.urlparse(url_a).netloc == urllib.parse.urlparse(url_b).netloc
    except Exception:
        return False

def fetch_html(session: requests.Session, url: str, timeout=(5, 20), max_retries=2) -> Optional[str]:
    for attempt in range(max_retries + 1):
        try:
            r = session.get(url, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            if r.status_code in (429, 503) and attempt < max_retries:
                time.sleep(1.2 * (attempt + 1))
                continue
            return None
        except requests.RequestException:
            if attempt < max_retries:
                time.sleep(1.0 * (attempt + 1))
                continue
            return None
    return None

def clean_soup(soup: BeautifulSoup) -> None:
    for t in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
        try:
            t.decompose()
        except Exception:
            pass
    exclude_tokens = ["sidebar", "ranking", "recommend", "widget", "ad", "bread", "breadcrumb", "banner"]
    for t in soup.find_all(True):
        if not isinstance(t, Tag):
            continue
        attrs = getattr(t, "attrs", None)
        if not isinstance(attrs, dict):
            continue
        cls_list = attrs.get("class") or []
        if not isinstance(cls_list, (list, tuple)):
            cls_list = [str(cls_list)]
        cls = " ".join(map(str, cls_list)).lower()
        if any(tok in cls for tok in exclude_tokens):
            try:
                t.decompose()
            except Exception:
                pass

def extract_main_text(soup: BeautifulSoup, rule: Optional[SiteRule]) -> str:
    if rule:
        for sel in rule.content_selectors:
            try:
                node = soup.select_one(sel)
                if node:
                    return node.get_text("\n", strip=True)
            except Exception:
                continue
    return soup.get_text("\n", strip=True)

def split_text_into_chunks(text: str, chunk_size=8000, overlap=400):
    if not text:
        return
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        yield text[start:end]
        start = max(end - overlap, end)

def find_next_page_url(soup: BeautifulSoup, current_url: str, rule: Optional[SiteRule]) -> Optional[str]:
    link_next = soup.find("link", rel="next")
    if link_next and link_next.get("href") and is_valid_href(link_next["href"]):
        joined = urllib.parse.urljoin(current_url, link_next["href"])
        if same_domain(joined, current_url):
            return joined
    a_next = soup.find("a", rel=lambda v: v and "next" in str(v).lower(), href=True)
    if a_next and is_valid_href(a_next["href"]):
        joined = urllib.parse.urljoin(current_url, a_next["href"])
        if same_domain(joined, current_url):
            return joined
    tokens = rule.listing_next_hint_tokens if rule else ("æ¬¡ã¸", "æ¬¡ã®", "ã‚‚ã£ã¨è¦‹ã‚‹", "Next", "More")
    for a in soup.find_all("a", href=True):
        try:
            txt = a.get_text(strip=True)
        except Exception:
            continue
        if any(t in txt for t in tokens):
            href = a.get("href")
            if href and is_valid_href(href):
                joined = urllib.parse.urljoin(current_url, href)
                if same_domain(joined, current_url):
                    return joined
    return None

def is_article_url(url: str, rule: Optional[SiteRule]) -> bool:
    if not rule:
        return True
    pu = urllib.parse.urlparse(url)
    path = pu.path or ""
    low = path.lower()
    if any(low.startswith(p) for p in rule.deny_path_prefixes):
        return False
    return bool(rule.article_path_allow.search(path))

def extract_article_links_from_listing(
    soup: BeautifulSoup,
    current_url: str,
    rule: Optional[SiteRule],
    link_limit: int = 80
) -> List[str]:
    base = urllib.parse.urlparse(current_url)
    out: List[str] = []
    seen: Set[str] = set()
    for a in soup.find_all("a", href=True):
        href = a.get("href")
        if not is_valid_href(href):
            continue
        url = urllib.parse.urljoin(current_url, href)
        pu = urllib.parse.urlparse(url)
        if pu.netloc != base.netloc:
            continue
        if not is_article_url(url, rule):
            continue
        if url not in seen:
            seen.add(url)
            out.append(url)
        if len(out) >= link_limit:
            break
    return out

# ------------------------------------------------------------
# Release date & JSON-LD location extraction
# ------------------------------------------------------------
def extract_release_date(soup: BeautifulSoup) -> str:
    meta_selectors = [
        ("meta", {"property": "article:published_time"}),
        ("meta", {"property": "og:published_time"}),
        ("meta", {"name": "pubdate"}),
        ("meta", {"name": "publishdate"}),
        ("meta", {"name": "date"}),
        ("meta", {"name": "dc.date"}),
        ("meta", {"name": "DC.date"}),
        ("meta", {"itemprop": "datePublished"}),
    ]
    for tag_name, attrs in meta_selectors:
        m = soup.find(tag_name, attrs=attrs)
        if m and m.get("content"):
            return normalize_date(str(m["content"]))
    t = soup.find("time")
    if t:
        dt = t.get("datetime")
        if dt:
            return normalize_date(str(dt))
        txt = t.get_text(strip=True)
        if txt:
            return normalize_date(txt)
    return ""

def _as_list(x: Any) -> List[Any]:
    if x is None:
        return []
    return x if isinstance(x, list) else [x]

def extract_location_from_jsonld(soup: BeautifulSoup) -> Dict[str, str]:
    out = {"address": "", "latitude": "", "longitude": ""}
    for sc in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            raw = sc.string or sc.get_text(strip=True)
            if not raw:
                continue
            obj = json.loads(raw)
        except Exception:
            continue
        nodes: List[Any] = []
        for node in _as_list(obj):
            if isinstance(node, dict) and "@graph" in node:
                nodes.extend(_as_list(node.get("@graph")))
            else:
                nodes.append(node)
        for n in nodes:
            if not isinstance(n, dict):
                continue
            loc = n.get("location") or n.get("Place") or n.get("place")
            for loc_node in _as_list(loc):
                if not isinstance(loc_node, dict):
                    continue
                addr = loc_node.get("address")
                if isinstance(addr, dict):
                    parts = [
                        addr.get("addressRegion"),
                        addr.get("addressLocality"),
                        addr.get("streetAddress"),
                        addr.get("postalCode"),
                        addr.get("addressCountry"),
                    ]
                    addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                    if addr_text and not out["address"]:
                        out["address"] = addr_text
                elif isinstance(addr, str) and addr.strip() and not out["address"]:
                    out["address"] = addr.strip()
                geo = loc_node.get("geo")
                if isinstance(geo, dict):
                    lat = geo.get("latitude")
                    lon = geo.get("longitude")
                    if lat is not None and not out["latitude"]:
                        out["latitude"] = str(lat).strip()
                    if lon is not None and not out["longitude"]:
                        out["longitude"] = str(lon).strip()
            addr2 = n.get("address")
            if isinstance(addr2, dict) and not out["address"]:
                parts = [
                    addr2.get("addressRegion"),
                    addr2.get("addressLocality"),
                    addr2.get("streetAddress"),
                    addr2.get("postalCode"),
                ]
                addr_text = "".join([p for p in parts if isinstance(p, str) and p.strip()])
                if addr_text:
                    out["address"] = addr_text
            geo2 = n.get("geo")
            if isinstance(geo2, dict):
                lat = geo2.get("latitude")
                lon = geo2.get("longitude")
                if lat is not None and not out["latitude"]:
                    out["latitude"] = str(lat).strip()
                if lon is not None and not out["longitude"]:
                    out["longitude"] = str(lon).strip()
            if out["address"] or out["latitude"] or out["longitude"]:
                return out
    return out

# ------------------------------------------------------------
# Gemini extraction (RETRY LOGIC ADDED)
# ------------------------------------------------------------
def ai_extract_events_from_text(
    client: genai.Client,
    model_name: str,
    temperature: float,
    text: str,
    today: datetime.date,
    debug_mode: bool,
    gemini_error_counter: Dict[str, int],
    min_chunk_len: int = 120,
) -> List[Dict]:
    all_items: List[Dict] = []
    
    # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†
    for chunk in split_text_into_chunks(text, chunk_size=8000, overlap=400):
        if not chunk or len(chunk) < min_chunk_len:
            continue

        prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’JSONé…åˆ—ã§æ¼ã‚ŒãªãæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã€ç¾åœ¨æ—¥ä»˜: {today}ã€‘

[æŠ½å‡ºãƒ«ãƒ¼ãƒ«]
- æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå±•ç¤ºã€å‚¬äº‹ã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€å‹Ÿé›†ã€ç™ºè¡¨ä¼šã€ã‚»ãƒŸãƒŠãƒ¼ç­‰ï¼‰ã‚„ã€æ—¥æ™‚ãƒ»æœŸé–“ãƒ»å ´æ‰€ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å¯èƒ½ãªé™ã‚ŠæŠ½å‡ºã€‚
- çœç•¥å³ç¦ã€‚ãŸã ã—ã€Œä¼æ¥­ãƒ•ãƒƒã‚¿ãƒ»å•ã„åˆã‚ã›å…ˆãƒ†ãƒ³ãƒ—ãƒ¬ã€ãªã©ã®éã‚¤ãƒ™ãƒ³ãƒˆå®šå‹æ–‡ã¯ç„¡ç†ã«æ‹¾ã‚ãªã„ã€‚
- date_info ã¯æœ¬æ–‡ã®è¡¨è¨˜ã®ã¾ã¾ã§ã‚‚è‰¯ã„ãŒã€å¯èƒ½ãªã‚‰ YYYYå¹´MMæœˆDDæ—¥ / YYYY/MM/DD / æœŸé–“è¡¨ç¾ï¼ˆä¾‹: 2025å¹´01æœˆ01æ—¥ã€œ2025å¹´02æœˆ01æ—¥ï¼‰ã€‚
- address / latitude / longitude ã¯æœ¬æ–‡ã‹ã‚‰æ¨å®šã§ãã‚‹ç¯„å›²ã§ã‚ˆã„ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰ã€‚
- å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ï¼ˆèª¬æ˜æ–‡ã¯ç¦æ­¢ï¼‰ã€‚

[JSONå½¢å¼]
[
  {{
    "name": "ã‚¿ã‚¤ãƒˆãƒ«",
    "place": "å ´æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "address": "ä½æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "latitude": "ç·¯åº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "longitude": "çµŒåº¦ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "date_info": "æ—¥ä»˜ã‚„æœŸé–“ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "description": "æ¦‚è¦ï¼ˆçŸ­ã‚ã«ï¼‰"
  }}
]

æœ¬æ–‡:
{chunk}
"""
        # ========================================================
        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ (Max 3å›)
        # ========================================================
        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                res = client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        temperature=float(temperature)
                    )
                )

                if debug_mode:
                    st.write("ğŸ§ª Gemini raw (head 400):", (res.text or "")[:400])

                extracted = safe_json_parse(res.text)
                if isinstance(extracted, list):
                    for item in extracted:
                        if not item or not isinstance(item, dict):
                            continue
                        name = str(item.get("name") or "").strip()
                        if not name:
                            continue
                        out = {
                            "name": name,
                            "place": str(item.get("place") or "").strip(),
                            "address": str(item.get("address") or "").strip(),
                            "latitude": str(item.get("latitude") or "").strip(),
                            "longitude": str(item.get("longitude") or "").strip(),
                            "date_info": normalize_date(str(item.get("date_info") or "").strip()),
                            "description": str(item.get("description") or "").strip(),
                        }
                        all_items.append(out)
                
                # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break 

            except Exception as e:
                # 429ã‚¨ãƒ©ãƒ¼ (Resource Exhausted) ã®å ´åˆã®ã¿å¾…æ©Ÿã—ã¦å†é–‹
                err_str = str(e)
                if "429" in err_str or "RESOURCE_EXHAUSTED" in err_str:
                    if attempt < max_retries:
                        wait_time = 10 * (attempt + 1) # 10s, 20s, 30s
                        if debug_mode:
                            st.warning(f"âš ï¸ 429 Detected. Retrying in {wait_time}s... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                        if debug_mode:
                            st.error(f"âŒ Retry Limit Reached: {e}")
                else:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å³æ™‚è¨˜éŒ²ã—ã¦æ¬¡ã¸
                    gemini_error_counter["count"] = gemini_error_counter.get("count", 0) + 1
                    if debug_mode:
                        st.error(f"âŒ Gemini Error: {e}")
                    else:
                        # 404ç­‰ã®å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ°—ã¥ã‹ã›ã‚‹
                        st.warning(f"âŒ Gemini Error: {e} (Check Model Name!)")
                    break

    return all_items

# ============================================================
# Sidebar UI
# ============================================================
with st.sidebar:
    st.header("1. å¯¾è±¡ã‚µã‚¤ãƒˆ")

    PRESET_URLS = {
        "PRTIMES (ã‚°ãƒ«ãƒ¡)": "https://prtimes.jp/gourmet/",
        "PRTIMES (ã‚¨ãƒ³ã‚¿ãƒ¡)": "https://prtimes.jp/entertainment/",
        "AtPress (ã‚°ãƒ«ãƒ¡)": "https://www.atpress.ne.jp/news/food",
        "AtPress (æ–°ç€)": "https://www.atpress.ne.jp/news",
    }

    selected_presets = st.multiselect(
        "ãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (ã‚°ãƒ«ãƒ¡)"],
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URLï¼ˆåŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ä¸€è¦§URLæ¨å¥¨ï¼‰")
    custom_urls_text = st.text_area("URLï¼ˆ1è¡Œã«1ã¤ï¼‰", height=110)

    st.divider()
    st.header("2. æ¢ç´¢è¨­å®š")
    max_pages = st.slider("ä¸€è¦§ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒšãƒ¼ã‚¸é€ã‚Šå›æ•°ï¼‰", 1, 30, 6)
    link_limit_per_page = st.slider("1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šåé›†ã™ã‚‹è¨˜äº‹URLä¸Šé™", 10, 300, 80, step=10)
    max_articles_total = st.slider("ç·è¨˜äº‹æ•°ã®ä¸Šé™ï¼ˆå®‰å…¨ç­–ï¼‰", 20, 2000, 400, step=20)
    # â–¼â–¼â–¼ ä¸Šé™ã‚’30ç§’ã«å¤‰æ›´ã—ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚’1ç§’åˆ»ã¿ã«ã—ã¾ã—ãŸ â–¼â–¼â–¼
    sleep_sec = st.slider("ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ï¼ˆç§’ï¼‰", 0.0, 30.0, 10.0, step=1.0)

    st.divider()
    st.header("3. Geminiè¨­å®š")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å®‰å®šç‰ˆã«æˆ»ã—ã¤ã¤ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¤‰æ›´å¯èƒ½ã«ã™ã‚‹
    model_name = st.text_input("ãƒ¢ãƒ‡ãƒ«å", value="gemini-1.5-flash")
    temperature = st.slider("temperatureï¼ˆ0æ¨å¥¨ï¼‰", 0.0, 1.0, 0.0, step=0.1)

    st.divider()
    st.header("ğŸ ãƒ‡ãƒãƒƒã‚°")
    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆè©³ç´°ãƒ­ã‚°è¡¨ç¤ºï¼‰", value=False)
    debug_show_articles = st.slider("ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã™ã‚‹è¨˜äº‹æ•°", 1, 10, 3)

    st.divider()
    st.header("4. æ—¢å­˜CSVã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–")
    uploaded_file = st.file_uploader("éå»CSVï¼ˆé‡è¤‡é™¤å¤–ç”¨ï¼‰", type="csv")
    
    # --------------------------------------------------------
    # NEW: ãƒ¢ãƒ‡ãƒ«åè¨ºæ–­ãƒ„ãƒ¼ãƒ« (ä¿®æ­£ç‰ˆï¼šã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ¼ãƒ‰)
    # --------------------------------------------------------
    st.divider()
    st.header("ğŸ” ãƒ¢ãƒ‡ãƒ«åè¨ºæ–­")
    if st.button("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"):
        api_key_check = None
        try:
            api_key_check = st.secrets["GOOGLE_API_KEY"]
        except:
            api_key_check = os.environ.get("GOOGLE_API_KEY")
            
        if not api_key_check:
            st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            try:
                tmp_client = genai.Client(api_key=api_key_check)
                # ãƒªã‚¹ãƒˆå–å¾—
                models_iter = tmp_client.models.list()
                
                valid_models = []
                for m in models_iter:
                    # å±æ€§ãƒã‚§ãƒƒã‚¯ã‚’å³å¯†ã«ã›ãšã€nameãŒã‚ã‚Œã°å–å¾—ã™ã‚‹
                    # æ–°SDKã§ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ getattr ã§å®‰å…¨ç­–ã‚’ã¨ã‚‹
                    raw_name = getattr(m, "name", str(m))
                    
                    # "models/" ã‚’å‰Šé™¤ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹
                    clean_name = raw_name.replace("models/", "")
                    
                    # Geminiç³»ã‹ã¤Vision/Contentç”Ÿæˆã§ããã†ãªã‚‚ã®ã ã‘æ®‹ã™ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿
                    if "gemini" in clean_name.lower():
                        valid_models.append(clean_name)
                
                if valid_models:
                    st.success("âœ… å–å¾—æˆåŠŸï¼ä»¥ä¸‹ã®åå‰ã‚’è©¦ã—ã¦ãã ã•ã„")
                    st.code("\n".join(sorted(valid_models)), language="text")
                else:
                    st.warning("ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            except Exception as e:
                st.error(f"ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# Load existing fingerprints
# ============================================================
existing_fingerprints: Set[Tuple[str, str]] = set()
if uploaded_file is not None:
    try:
        existing_df = pd.read_csv(uploaded_file)
        name_col = next((c for c in existing_df.columns if "ã‚¤ãƒ™ãƒ³ãƒˆå" in c or c.lower() in ["name", "title"]), None)
        place_col = next((c for c in existing_df.columns if "å ´æ‰€" in c or c.lower() in ["place", "location"]), None)

        if name_col:
            for _, row in existing_df.iterrows():
                n = normalize_string(row.get(name_col, ""))
                p = normalize_string(row.get(place_col, "")) if place_col else ""
                if n:
                    existing_fingerprints.add((n, p))
            st.sidebar.success(f"ğŸ“š {len(existing_fingerprints)}ä»¶ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
        else:
            st.sidebar.warning("CSVã«ã‚¤ãƒ™ãƒ³ãƒˆååˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡é™¤å¤–ãªã—ã§ç¶šè¡Œï¼‰ã€‚")
    except Exception as e:
        st.sidebar.error(f"CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# Session state
# ============================================================
if "extracted_data" not in st.session_state:
    st.session_state.extracted_data = None
if "last_update" not in st.session_state:
    st.session_state.last_update = None

# ============================================================
# Main
# ============================================================
if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    api_key = None
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except Exception:
        api_key = os.environ.get("GOOGLE_API_KEY")

    if not api_key:
        st.error("âš ï¸ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆst.secrets ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ï¼‰ã€‚")
        st.stop()

    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})

    if custom_urls_text:
        for u in custom_urls_text.splitlines():
            u = u.strip()
            if u.startswith("http"):
                domain = urllib.parse.urlparse(u).netloc
                targets.append({"url": u, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})

    unique_targets = {t["url"]: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    today = datetime.date.today()
    client = genai.Client(api_key=api_key)

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    })

    status = st.empty()
    progress = st.progress(0.0)

    # --------------------------------------------------------
    # 1) Collect article URLs from listings
    # --------------------------------------------------------
    collected: List[Tuple[str, str]] = []
    collected_seen: Set[str] = set()
    visited_listing: Set[str] = set()

    total_units = max(len(targets) * max_pages, 1)
    done_units = 0

    for target in targets:
        base_url = target["url"]
        label = target["label"]
        current_url = base_url
        rule = get_site_rule(current_url)

        for page_num in range(1, max_pages + 1):
            done_units += 1
            progress.progress(min(done_units / total_units, 1.0))

            if current_url in visited_listing:
                status.warning(f"ğŸ” ä¸€è¦§URLå†è¨ªã®ãŸã‚åœæ­¢: {current_url}")
                break
            visited_listing.add(current_url)

            status.info(f"ğŸ“„ ä¸€è¦§å–å¾—: {label} | {page_num}/{max_pages}\n{current_url}")

            html = fetch_html(session, current_url)
            if not html:
                status.warning(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {current_url}")
                break

            soup = BeautifulSoup(html, "html.parser")
            next_url = find_next_page_url(soup, current_url, rule)
            links = extract_article_links_from_listing(soup, current_url, rule, link_limit=link_limit_per_page)

            add_count = 0
            for u in links:
                if u not in collected_seen:
                    collected_seen.add(u)
                    collected.append((u, label))
                    add_count += 1

            status.info(f"ğŸ”— è¨˜äº‹URLåé›†: +{add_count}ä»¶ï¼ˆç´¯è¨ˆ {len(collected)}ä»¶ï¼‰")

            if len(collected) >= max_articles_total:
                break
            if not next_url:
                break

            current_url = next_url
            time.sleep(sleep_sec)

        if len(collected) >= max_articles_total:
            break

    collected = collected[:max_articles_total]

    if not collected:
        progress.empty()
        status.error("ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.session_state.extracted_data = None
        st.stop()

    # --------------------------------------------------------
    # 2) Extract events from article pages
    # --------------------------------------------------------
    status.info(f"ğŸ§  è¨˜äº‹ãƒšãƒ¼ã‚¸è§£æé–‹å§‹ï¼ˆç· {len(collected)} ä»¶ï¼‰")
    extracted_all: List[Dict] = []

    run_fingerprints: Set[Tuple[str, str]] = set()

    skipped_duplicate_csv = 0
    skipped_duplicate_run = 0
    failed_articles = 0
    non_article_skipped = 0
    short_text_skipped = 0
    gemini_error_counter = {"count": 0}

    for i, (article_url, label) in enumerate(collected, start=1):
        progress.progress(min(i / max(len(collected), 1), 1.0))
        status.info(f"ğŸ§  è¨˜äº‹è§£æ {i}/{len(collected)}: {article_url}")

        rule = get_site_rule(article_url)

        if not is_article_url(article_url, rule):
            non_article_skipped += 1
            continue

        html = fetch_html(session, article_url)
        if not html:
            failed_articles += 1
            continue

        soup = BeautifulSoup(html, "html.parser")

        release_date = extract_release_date(soup)
        loc = extract_location_from_jsonld(soup)

        clean_soup(soup)
        text = extract_main_text(soup, rule)

        if not text or len(text) < 200:
            short_text_skipped += 1
            if debug_mode and i <= debug_show_articles:
                st.warning(f"æœ¬æ–‡ãŒçŸ­ã™ãã¦ã‚¹ã‚­ãƒƒãƒ—: len={len(text) if text else 0} url={article_url}")
                st.write((text or "")[:500])
            continue

        if debug_mode and i <= debug_show_articles:
            st.write(f"ğŸ§ª [debug] text length={len(text)} release_date={release_date}")
            st.write("ğŸ§ª [debug] text head:", text[:500])
            if loc.get("address") or loc.get("latitude") or loc.get("longitude"):
                st.write("ğŸ§ª [debug] jsonld loc:", loc)

        items = ai_extract_events_from_text(
            client=client,
            model_name=model_name,
            temperature=temperature,
            text=text,
            today=today,
            debug_mode=debug_mode and (i <= debug_show_articles),
            gemini_error_counter=gemini_error_counter,
        )

        for item in items:
            item["release_date"] = release_date

            if loc.get("address") and not item.get("address"):
                item["address"] = loc["address"]
            if loc.get("latitude") and not item.get("latitude"):
                item["latitude"] = loc["latitude"]
            if loc.get("longitude") and not item.get("longitude"):
                item["longitude"] = loc["longitude"]

            n = normalize_string(item.get("name", ""))
            p = normalize_string(item.get("place", ""))

            if not n:
                continue

            fp = (n, p)

            if fp in existing_fingerprints:
                skipped_duplicate_csv += 1
                continue

            if fp in run_fingerprints:
                skipped_duplicate_run += 1
                continue

            run_fingerprints.add(fp)

            item["source_label"] = label
            item["source_url"] = article_url
            extracted_all.append(item)

        time.sleep(sleep_sec)

    progress.empty()

    if gemini_error_counter.get("count", 0) > 0:
        st.warning(
            f"âš ï¸ Geminiå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒ {gemini_error_counter['count']} å›ç™ºç”Ÿã—ã¾ã—ãŸã€‚"
            f"ï¼ˆãƒ¢ãƒ‡ãƒ«å/ã‚¯ã‚©ãƒ¼ã‚¿/APIã‚­ãƒ¼æ¨©é™ã®å¯èƒ½æ€§ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨ºæ–­ãƒ„ãƒ¼ãƒ«ã‚‚æ´»ç”¨ã—ã¦ãã ã•ã„ï¼‰"
        )

    if not extracted_all:
        status.warning(
            f"æŠ½å‡ºçµæœãŒ0ä»¶ã§ã—ãŸã€‚\n\n"
            f"- è¨˜äº‹å¤±æ•—: {failed_articles}ä»¶\n"
            f"- éè¨˜äº‹URLã‚¹ã‚­ãƒƒãƒ—: {non_article_skipped}ä»¶\n"
            f"- æœ¬æ–‡çŸ­ã™ãã‚¹ã‚­ãƒƒãƒ—: {short_text_skipped}ä»¶\n"
            f"- CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶\n"
            f"- Geminiã‚¨ãƒ©ãƒ¼: {gemini_error_counter.get('count', 0)}ä»¶"
        )
        st.session_state.extracted_data = None
        st.stop()

    st.session_state.extracted_data = extracted_all
    st.session_state.last_update = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    status.success(
        f"ğŸ‰ å®Œäº†ï¼æ–°è¦ {len(extracted_all)} ä»¶\n"
        f"- CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶\n"
        f"- ä»Šå›é‡è¤‡é™¤å¤–: {skipped_duplicate_run}ä»¶\n"
        f"- éè¨˜äº‹URLã‚¹ã‚­ãƒƒãƒ—: {non_article_skipped}ä»¶\n"
        f"- æœ¬æ–‡çŸ­ã™ãã‚¹ã‚­ãƒƒãƒ—: {short_text_skipped}ä»¶\n"
        f"- è¨˜äº‹å¤±æ•—: {failed_articles}ä»¶\n"
        f"- Geminiã‚¨ãƒ©ãƒ¼: {gemini_error_counter.get('count', 0)}ä»¶"
    )
# ============================================================
# Result rendering
# ============================================================
if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)

    st.markdown(f"**å–å¾—ä»¶æ•°: {len(df)}**ï¼ˆæ›´æ–°: {st.session_state.last_update}ï¼‰")

    display_df = df.rename(columns={
        "release_date": "ãƒªãƒªãƒ¼ã‚¹æ—¥",
        "date_info": "æœŸé–“",
        "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
        "place": "å ´æ‰€",
        "address": "ä½æ‰€",
        "latitude": "ç·¯åº¦",
        "longitude": "çµŒåº¦",
        "description": "æ¦‚è¦",
        "source_label": "æƒ…å ±æº",
        "source_url": "URL"
    })

    desired_cols = ["ãƒªãƒªãƒ¼ã‚¹æ—¥", "æœŸé–“", "ã‚¤ãƒ™ãƒ³ãƒˆå", "å ´æ‰€", "ä½æ‰€", "ç·¯åº¦", "çµŒåº¦", "æ¦‚è¦", "æƒ…å ±æº", "URL"]
    cols = [c for c in desired_cols if c in display_df.columns]
    display_df = display_df[cols]

    # â–¼â–¼â–¼ è¿½åŠ æ©Ÿèƒ½ï¼šã‚´ãƒŸæƒé™¤ï¼ˆã€Œç©ºæ–‡å­—ã€ã‚„ã€Œä¸æ˜ã€ã‚’æœ¬å½“ã®ç©ºæ¬„ã«ã™ã‚‹ï¼‰ â–¼â–¼â–¼
    # AIãŒæ–‡å­—é€šã‚Šã€Œç©ºæ–‡å­—ã€ã¨æ›¸ã„ã¦ã—ã¾ã£ãŸã‚Šã€ã€Œä¸æ˜ã€ã¨å…¥ã‚ŒãŸã‚Šã™ã‚‹ã®ã‚’æ¶ˆå»
    rubbish_words = ["ç©ºæ–‡å­—", "ä¸æ˜", "None", "null", "N/A", "æœªå®š"]
    for col in display_df.columns:
        # æ–‡å­—åˆ—å‹ã®åˆ—ã«å¯¾ã—ã¦ç½®æ›ã‚’å®Ÿè¡Œ
        display_df[col] = display_df[col].replace(rubbish_words, "")
    # â–²â–²â–² è¿½åŠ çµ‚äº† â–²â–²â–²

    if "æœŸé–“" in display_df.columns:
        try:
            display_df = display_df.sort_values("æœŸé–“", na_position="last")
        except Exception:
            pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— Link"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large"),
        },
        hide_index=True
    )

    csv_bytes = display_df.to_csv(index=False).encode("utf-8_sig")
    st.download_button("ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_bytes, "events_full.csv", "text/csv")
