import streamlit as st
import datetime
import os
import json
import time
import re
import urllib.parse
from typing import List, Dict, Tuple, Optional, Set

import pandas as pd
import requests
from bs4 import BeautifulSoup

from google import genai
from google.genai import types

# ============================================================
# Streamlit config
# ============================================================
st.set_page_config(page_title="ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆå®Œæˆç‰ˆï¼‰", page_icon="ğŸ“–", layout="wide")
st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå…¨ä»¶ç¶²ç¾…ã€æŠ½å‡ºã‚¢ãƒ—ãƒªï¼ˆå®Œæˆç‰ˆï¼‰")
st.markdown("""
**AI Ã— ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå®Œæˆç‰ˆï¼‰**  
ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ã‚«ãƒ¼ãƒ‰ã‹ã‚‰**è¨˜äº‹URLã‚’åé›†** â†’ è¨˜äº‹ãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’**AIã§æŠ½å‡º**ã—ã€é‡è¤‡ã‚’é™¤å¤–ã—ã¦ä¸€è¦§åŒ–ã—ã¾ã™ã€‚
""")

# ============================================================
# Utils
# ============================================================

def normalize_date(text: str) -> str:
    """æ—¥ä»˜ã‚’ã‚¼ãƒ­åŸ‹ã‚ã§ãªã‚‹ã¹ãæƒãˆã‚‹ï¼ˆæ–‡å­—åˆ—ã®ã¾ã¾æ‰±ã†ï¼‰"""
    if not text or not isinstance(text, str):
        return ""

    # 2025å¹´1æœˆ2æ—¥ -> 2025å¹´01æœˆ02æ—¥
    def rep_ymd(m):
        return f"{m.group(1)}å¹´{m.group(2).zfill(2)}æœˆ{m.group(3).zfill(2)}æ—¥"
    text = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", rep_ymd, text)

    # 2025/1/2 -> 2025/01/02
    text = re.sub(
        r"(\d{4})/(\d{1,2})/(\d{1,2})",
        lambda m: f"{m.group(1)}/{m.group(2).zfill(2)}/{m.group(3).zfill(2)}",
        text
    )

    return text.strip()

def normalize_string(text) -> str:
    if not isinstance(text, str):
        return ""
    t = text.replace(" ", "").replace("ã€€", "")
    t = t.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return t.lower().strip()

def safe_json_parse(json_str: str) -> List[Dict]:
    """Geminiå‡ºåŠ›ã®æºã‚Œã«è€ãˆã‚‹JSONæ•‘å‡ºã€‚ãƒªã‚¹ãƒˆæŠ½å‡ºâ†’è¾æ›¸æŠ½å‡ºã®é †ã€‚"""
    if not json_str or not isinstance(json_str, str):
        return []
    s = json_str.replace("```json", "").replace("```", "").strip()

    # list candidate
    l = s.find("[")
    r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return obj if isinstance(obj, list) else []
        except:
            pass

    # dict candidate
    l = s.find("{")
    r = s.rfind("}")
    if l != -1 and r != -1 and r > l:
        cand = s[l:r+1]
        try:
            obj = json.loads(cand)
            return [obj] if isinstance(obj, dict) else []
        except:
            pass

    return []

def clean_soup(soup: BeautifulSoup) -> None:
    """ä¸è¦è¦ç´ ã‚’å‰Šé™¤ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’å®‰å®šã•ã›ã‚‹"""
    for tag in soup.find_all(["script", "style", "nav", "footer", "iframe", "header", "noscript", "svg"]):
        tag.decompose()

    exclude_tokens = ["sidebar", "ranking", "recommend", "widget", "ad", "bread", "breadcrumb", "banner"]
    for tag in soup.find_all(attrs={"class": True}):
        cls_list = tag.get("class") or []
        cls = " ".join(cls_list).lower()
        if any(tok in cls for tok in exclude_tokens):
            tag.decompose()

def split_text_into_chunks(text: str, chunk_size=8000, overlap=400):
    if not text:
        return
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        yield text[start:end]
        start = max(end - overlap, end)

def is_valid_href(href: str) -> bool:
    if not href:
        return False
    h = href.strip()
    if h.startswith("#"):
        return False
    if h.lower().startswith("javascript:"):
        return False
    return True

def same_domain(url_a: str, url_b: str) -> bool:
    try:
        return urllib.parse.urlparse(url_a).netloc == urllib.parse.urlparse(url_b).netloc
    except:
        return False

def find_next_page_url(soup: BeautifulSoup, current_url: str) -> Optional[str]:
    """ã€Œæ¬¡ã¸ã€ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒªãƒ³ã‚¯ã‚’æ¢ã™ã€‚èª¤çˆ†ã‚’æ¸›ã‚‰ã™ãŸã‚å„ªå…ˆé †ã‚’ã¤ã‘ã‚‹ã€‚"""
    next_url = None

    # 1) rel=next
    link_next = soup.find("link", rel="next")
    if link_next and link_next.get("href") and is_valid_href(link_next["href"]):
        next_url = link_next["href"]

    # 2) æ˜ç¤ºãƒœã‚¿ãƒ³ï¼ˆã‚ˆãã‚ã‚‹ï¼‰
    if not next_url:
        selectors = [
            "a[rel='next']",
            "a.next",
            "a.pagination__next",
            "a.pager-next",
            "a:contains('æ¬¡ã¸')",
        ]
        # BeautifulSoupã¯:containsãŒåŠ¹ã‹ãªã„ã®ã§ã€ãƒ†ã‚­ã‚¹ãƒˆå«ã¿ã§æ‹¾ã†
        for a in soup.find_all("a", href=True):
            txt = a.get_text(strip=True)
            cls = " ".join(a.get("class") or []).lower()
            if any(k in txt for k in ["æ¬¡ã¸", "æ¬¡ã®", "Next", "NEXT"]) or any(k in cls for k in ["next", "more"]):
                href = a.get("href")
                if href and is_valid_href(href):
                    next_url = href
                    break

    if next_url:
        joined = urllib.parse.urljoin(current_url, next_url)
        # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³å„ªå…ˆï¼ˆé•ã†ãªã‚‰ç„¡åŠ¹æ‰±ã„ï¼‰
        if same_domain(joined, current_url):
            return joined
    return None

def extract_article_links_from_listing(
    soup: BeautifulSoup,
    current_url: str,
    link_limit: int = 50
) -> List[str]:
    """
    ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLå€™è£œã‚’æŠ½å‡ºã€‚
    - ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®ã¿
    - æ˜ã‚‰ã‹ã«ä¸€è¦§ã‚„ã‚¿ã‚°ã€ãƒ­ã‚°ã‚¤ãƒ³ç­‰ã¯é™¤å¤–
    """
    base = urllib.parse.urlparse(current_url)
    candidates: List[str] = []

    # ã¾ãš "è¨˜äº‹ã‚«ãƒ¼ãƒ‰ã£ã½ã„" aã‚¿ã‚°ã‹ã‚‰å¤šã‚ã«æ‹¾ã†ï¼ˆæ±ç”¨ï¼‰
    for a in soup.find_all("a", href=True):
        href = a.get("href")
        if not is_valid_href(href):
            continue
        url = urllib.parse.urljoin(current_url, href)
        pu = urllib.parse.urlparse(url)

        # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é™å®š
        if pu.netloc != base.netloc:
            continue

        # ã‚ã‚ŠãŒã¡ãªé™¤å¤–
        path = (pu.path or "").lower()
        if any(x in path for x in ["/tag/", "/tags/", "/category/", "/categories/", "/login", "/signup", "/account"]):
            continue

        # URLãŒçŸ­ã™ãã‚‹/ãƒˆãƒƒãƒ—ã£ã½ã„ã®ã¯é™¤å¤–
        if len(path.strip("/")) < 3:
            continue

        candidates.append(url)

    # é‡è¤‡æ’é™¤ï¼ˆé †åºä¿æŒï¼‰
    seen = set()
    uniq = []
    for u in candidates:
        if u not in seen:
            seen.add(u)
            uniq.append(u)

    return uniq[:link_limit]

def fetch_html(session: requests.Session, url: str, timeout=(5, 20), max_retries=2) -> Optional[str]:
    """è»½ã„ãƒªãƒˆãƒ©ã‚¤ä»˜ãHTMLå–å¾—"""
    for attempt in range(max_retries + 1):
        try:
            r = session.get(url, timeout=timeout)
            if r.status_code == 200 and r.text:
                return r.text
            # 429/503ã ã‘å°‘ã—å¾…ã£ã¦å†è©¦è¡Œ
            if r.status_code in (429, 503) and attempt < max_retries:
                time.sleep(1.2 * (attempt + 1))
                continue
            return None
        except requests.RequestException:
            if attempt < max_retries:
                time.sleep(1.0 * (attempt + 1))
                continue
            return None
    return None

def ai_extract_events_from_text(
    client: genai.Client,
    model_name: str,
    text: str,
    today: datetime.date,
) -> List[Dict]:
    """è¨˜äº‹æœ¬æ–‡ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
    all_items: List[Dict] = []

    for chunk in split_text_into_chunks(text, chunk_size=8000, overlap=400):
        if not chunk or len(chunk) < 80:
            continue

        prompt = f"""
ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã‹ã‚‰ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’JSONé…åˆ—ã§æ¼ã‚ŒãªãæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã€ç¾åœ¨æ—¥ä»˜: {today}ã€‘

[æŠ½å‡ºãƒ«ãƒ¼ãƒ«]
- æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå±•ç¤ºã€å‚¬äº‹ã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã€å‹Ÿé›†ã€ç™ºè¡¨ä¼šã€ã‚»ãƒŸãƒŠãƒ¼ç­‰ï¼‰ã‚„ã€æ—¥æ™‚ãƒ»æœŸé–“ãƒ»å ´æ‰€ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’å¯èƒ½ãªé™ã‚ŠæŠ½å‡ºã€‚
- çœç•¥å³ç¦ï¼ˆãŸã ã—ã€Œæ˜ã‚‰ã‹ã«ã‚¤ãƒ™ãƒ³ãƒˆã§ã¯ãªã„å®šå‹ãƒ•ãƒƒã‚¿ã€ãªã©ã¯ç„¡ç†ã«æ‹¾ã‚ãªã„ï¼‰ã€‚
- date_info ã¯æœ¬æ–‡ã®è¡¨è¨˜ã®ã¾ã¾ã§ã‚‚è‰¯ã„ãŒã€å¯èƒ½ãªã‚‰ YYYYå¹´MMæœˆDDæ—¥ / YYYY/MM/DD / æœŸé–“è¡¨ç¾ï¼ˆä¾‹: 2025å¹´01æœˆ01æ—¥ã€œ2025å¹´02æœˆ01æ—¥ï¼‰ã®ã‚ˆã†ã«åˆ†ã‹ã‚‹å½¢ã§å…¥ã‚Œã‚‹ã€‚
- å‡ºåŠ›ã¯å¿…ãš JSON ã®ã¿ã€‚

[JSONå½¢å¼]
[
  {{
    "name": "ã‚¿ã‚¤ãƒˆãƒ«",
    "place": "å ´æ‰€ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "date_info": "æ—¥ä»˜ã‚„æœŸé–“ï¼ˆä¸æ˜ãªã‚‰ç©ºæ–‡å­—ï¼‰",
    "description": "æ¦‚è¦ï¼ˆçŸ­ã‚ã«ï¼‰"
  }}
]

æœ¬æ–‡:
{chunk}
"""
        try:
            res = client.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.0
                )
            )
            extracted = safe_json_parse(res.text)
            if isinstance(extracted, list):
                for item in extracted:
                    if not item or not isinstance(item, dict):
                        continue
                    name = item.get("name") or ""
                    if not name.strip():
                        continue
                    item["name"] = str(name).strip()
                    item["place"] = str(item.get("place") or "").strip()
                    item["date_info"] = normalize_date(str(item.get("date_info") or "").strip())
                    item["description"] = str(item.get("description") or "").strip()
                    all_items.append(item)
        except Exception:
            continue

    return all_items

# ============================================================
# Sidebar
# ============================================================
with st.sidebar:
    st.header("1. å¯¾è±¡ã‚µã‚¤ãƒˆ")

    PRESET_URLS = {
        "PRTIMES (ã‚°ãƒ«ãƒ¡)": "https://prtimes.jp/gourmet/",
        "PRTIMES (ã‚¨ãƒ³ã‚¿ãƒ¡)": "https://prtimes.jp/entertainment/",
        "AtPress (ã‚°ãƒ«ãƒ¡)": "https://www.atpress.ne.jp/news/food",
        "AtPress (æ–°ç€)": "https://www.atpress.ne.jp/news",
    }

    selected_presets = st.multiselect(
        "ãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (ã‚°ãƒ«ãƒ¡)"],
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area("URLï¼ˆ1è¡Œã«1ã¤ï¼‰", height=110)

    st.divider()
    st.header("2. æ¢ç´¢è¨­å®š")
    max_pages = st.slider("ä¸€è¦§ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒšãƒ¼ã‚¸é€ã‚Šå›æ•°ï¼‰", 1, 20, 5)
    link_limit_per_page = st.slider("1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šåé›†ã™ã‚‹è¨˜äº‹URLä¸Šé™", 10, 200, 60, step=10)
    max_articles_total = st.slider("ç·è¨˜äº‹æ•°ã®ä¸Šé™ï¼ˆå®‰å…¨ç­–ï¼‰", 20, 1000, 250, step=10)
    sleep_sec = st.slider("ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ï¼ˆç§’ï¼‰", 0.0, 2.0, 0.6, step=0.1)

    st.divider()
    st.header("3. Geminiè¨­å®š")
    model_name = st.text_input("ãƒ¢ãƒ‡ãƒ«å", value="gemini-2.0-flash")
    temperature = st.slider("temperatureï¼ˆé€šå¸¸0æ¨å¥¨ï¼‰", 0.0, 1.0, 0.0, step=0.1)

    st.divider()
    st.header("4. æ—¢å­˜CSVã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–")
    uploaded_file = st.file_uploader("éå»CSVï¼ˆé‡è¤‡é™¤å¤–ç”¨ï¼‰", type="csv")

    existing_fingerprints: Set[Tuple[str, str]] = set()
    if uploaded_file is not None:
        try:
            existing_df = pd.read_csv(uploaded_file)
            name_col = next((c for c in existing_df.columns if "ã‚¤ãƒ™ãƒ³ãƒˆå" in c or c.lower() in ["name", "title"]), None)
            place_col = next((c for c in existing_df.columns if "å ´æ‰€" in c or c.lower() in ["place", "location"]), None)

            if name_col:
                for _, row in existing_df.iterrows():
                    n = normalize_string(row.get(name_col, ""))
                    p = normalize_string(row.get(place_col, "")) if place_col else ""
                    if n:
                        existing_fingerprints.add((n, p))
                st.success(f"ğŸ“š {len(existing_fingerprints)}ä»¶ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰")
            else:
                st.warning("CSVã«ã‚¤ãƒ™ãƒ³ãƒˆååˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆé‡è¤‡é™¤å¤–ãªã—ã§ç¶šè¡Œï¼‰ã€‚")
        except Exception as e:
            st.error(f"CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# Session State
# ============================================================
if "extracted_data" not in st.session_state:
    st.session_state.extracted_data = None
if "last_update" not in st.session_state:
    st.session_state.last_update = None

# ============================================================
# Main logic
# ============================================================
if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    # API key
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except Exception:
        api_key = os.environ.get("GOOGLE_API_KEY")

    if not api_key:
        st.error("âš ï¸ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚st.secrets ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # targets
    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})

    if custom_urls_text:
        for u in custom_urls_text.splitlines():
            u = u.strip()
            if u.startswith("http"):
                domain = urllib.parse.urlparse(u).netloc
                targets.append({"url": u, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})

    unique_targets = {t["url"]: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    today = datetime.date.today()

    client = genai.Client(api_key=api_key)

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    })

    main_progress = st.progress(0.0)
    status_box = st.empty()

    # gathering article urls
    all_article_urls: List[Tuple[str, str]] = []  # (article_url, source_label)
    visited_listing: Set[str] = set()

    total_units = len(targets) * max_pages
    unit_done = 0

    for target in targets:
        base_url = target["url"]
        label = target["label"]
        current_url = base_url

        for page_num in range(1, max_pages + 1):
            unit_done += 1
            main_progress.progress(min(unit_done / max(total_units, 1), 1.0))

            if current_url in visited_listing:
                status_box.warning(f"ğŸ” æ—¢ã«è¨ªå•æ¸ˆã¿ã®ä¸€è¦§URLã®ãŸã‚åœæ­¢: {current_url}")
                break
            visited_listing.add(current_url)

            status_box.info(f"ğŸ“„ ä¸€è¦§å–å¾—: {label} | {page_num}ãƒšãƒ¼ã‚¸ç›®\n{current_url}")

            html = fetch_html(session, current_url)
            if not html:
                status_box.warning(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {current_url}")
                break

            soup = BeautifulSoup(html, "html.parser")
            next_url = find_next_page_url(soup, current_url)

            # ä¸€è¦§ã‹ã‚‰è¨˜äº‹ãƒªãƒ³ã‚¯åé›†
            links = extract_article_links_from_listing(
                soup, current_url, link_limit=link_limit_per_page
            )
            for u in links:
                all_article_urls.append((u, label))

            # ä¸Šé™å®‰å…¨ç­–
            if len(all_article_urls) >= max_articles_total:
                break

            if not next_url:
                break

            current_url = next_url
            time.sleep(sleep_sec)

        if len(all_article_urls) >= max_articles_total:
            break

    # article url de-dup
    dedup = []
    seen_url = set()
    for u, lab in all_article_urls:
        if u not in seen_url:
            seen_url.add(u)
            dedup.append((u, lab))
    all_article_urls = dedup[:max_articles_total]

    if not all_article_urls:
        main_progress.empty()
        status_box.error("ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹URLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.session_state.extracted_data = None
        st.stop()

    # =========================================================
    # Extract events from articles
    # =========================================================
    status_box.info(f"ğŸ§  è¨˜äº‹ãƒšãƒ¼ã‚¸è§£æé–‹å§‹ï¼ˆç·{len(all_article_urls)}ä»¶ï¼‰")
    extracted_all: List[Dict] = []
    visited_article: Set[str] = set()

    skipped_duplicate_csv = 0
    skipped_duplicate_run = 0
    failed_articles = 0

    for i, (article_url, label) in enumerate(all_article_urls, start=1):
        main_progress.progress(min(i / max(len(all_article_urls), 1), 1.0))
        status_box.info(f"ğŸ§  è¨˜äº‹è§£æ {i}/{len(all_article_urls)}: {article_url}")

        if article_url in visited_article:
            continue
        visited_article.add(article_url)

        html = fetch_html(session, article_url)
        if not html:
            failed_articles += 1
            continue

        soup = BeautifulSoup(html, "html.parser")
        clean_soup(soup)
        text = soup.get_text("\n", strip=True)

        # AIæŠ½å‡º
        items = ai_extract_events_from_text(client, model_name, text, today)

        # ä»˜å¸¯æƒ…å ±ï¼†é‡è¤‡é™¤å¤–
        for item in items:
            n = normalize_string(item.get("name", ""))
            p = normalize_string(item.get("place", ""))

            if not n:
                continue

            # CSVæ—¢çŸ¥é‡è¤‡é™¤å¤–
            if (n, p) in existing_fingerprints:
                skipped_duplicate_csv += 1
                continue

            # ä»Šå›å–å¾—å†…ã®é‡è¤‡é™¤å¤–ï¼ˆã‚½ãƒ¼ã‚¹å•ã‚ãšï¼‰
            fp = (n, p, normalize_string(item.get("date_info", ""))[:20])
            # date_infoã¾ã§å«ã‚ãŸè»½ã„æŒ‡ç´‹ï¼ˆå®Œå…¨ä¸€è‡´ã‚’é¿ã‘ã‚‹ï¼‰
            # ãŸã ã— name/place ãŒåŒã˜ãªã‚‰åŸºæœ¬åŒä¸€ã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦æ‰±ã„ãŸã„å ´åˆã¯ date_infoã‚’å¤–ã—ã¦ã‚‚OK
            # ã“ã“ã§ã¯ã€Œname+placeã€ã‚’æœ€å„ªå…ˆã«ã™ã‚‹
            fp2 = (n, p)

            # ã™ã§ã«æŠ½å‡ºæ¸ˆã¿ã‹ç¢ºèª
            exists = False
            for d in extracted_all:
                if (normalize_string(d.get("name","")), normalize_string(d.get("place",""))) == fp2:
                    exists = True
                    break
            if exists:
                skipped_duplicate_run += 1
                continue

            item["source_label"] = label
            item["source_url"] = article_url
            extracted_all.append(item)

        time.sleep(sleep_sec)

    main_progress.empty()

    if not extracted_all:
        status_box.warning(
            f"æŠ½å‡ºçµæœãŒ0ä»¶ã§ã—ãŸã€‚è¨˜äº‹å–å¾—å¤±æ•—: {failed_articles}ä»¶ / CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶"
        )
        st.session_state.extracted_data = None
        st.stop()

    st.session_state.extracted_data = extracted_all
    st.session_state.last_update = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    status_box.success(
        f"ğŸ‰ å®Œäº†ï¼æ–°è¦ {len(extracted_all)} ä»¶ / CSVé™¤å¤–: {skipped_duplicate_csv}ä»¶ / ä»Šå›é‡è¤‡é™¤å¤–: {skipped_duplicate_run}ä»¶ / è¨˜äº‹å¤±æ•—: {failed_articles}ä»¶"
    )

# ============================================================
# Result rendering
# ============================================================
if st.session_state.extracted_data:
    df = pd.DataFrame(st.session_state.extracted_data)

    st.markdown(f"**å–å¾—ä»¶æ•°: {len(df)}**ï¼ˆæ›´æ–°: {st.session_state.last_update}ï¼‰")

    # è¡¨ç¤ºç”¨ãƒªãƒãƒ¼ãƒ 
    display_df = df.rename(columns={
        "date_info": "æœŸé–“",
        "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
        "place": "å ´æ‰€",
        "description": "æ¦‚è¦",
        "source_label": "æƒ…å ±æº",
        "source_url": "URL"
    })

    # åˆ—ã®å­˜åœ¨ã‚’ä¿è¨¼ã—ã¤ã¤ä¸¦ã¹ã‚‹
    desired_cols = ["æœŸé–“", "ã‚¤ãƒ™ãƒ³ãƒˆå", "å ´æ‰€", "æ¦‚è¦", "æƒ…å ±æº", "URL"]
    cols = [c for c in desired_cols if c in display_df.columns]
    display_df = display_df[cols]

    # ã‚½ãƒ¼ãƒˆï¼ˆæœŸé–“ãŒç©ºã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã«ï¼‰
    if "æœŸé–“" in display_df.columns:
        try:
            display_df = display_df.sort_values("æœŸé–“", na_position="last")
        except:
            pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— Link"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large")
        },
        hide_index=True
    )

    csv_bytes = display_df.to_csv(index=False).encode("utf-8_sig")
    st.download_button("ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_bytes, "events_full.csv", "text/csv")
